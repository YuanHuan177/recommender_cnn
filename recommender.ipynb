{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recommender.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunflowersunflower/recommender_cnn/blob/master/recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qI_zm6mxCjj",
        "colab_type": "code",
        "outputId": "5a66b6b0-760c-4462-f318-80070a1cce60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "from tensorflow.python.ops import math_ops"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNO5HUx_1hZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NbY6hlg1rS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1qn0ljk89KlTT46fWIPBuJZZIyoqqB7rz'\n",
        "fluff, id = link.split('=')\n",
        "file = drive.CreateFile({'id':id}) \n",
        "file.GetContentFile('users.dat')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs8qT3zyxXFn",
        "colab_type": "code",
        "outputId": "231ab731-5f49-417e-8044-a1decfa11fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
        "users = pd.read_csv('users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
        "users_orig = users.values\n",
        "users.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>OccupationID</th>\n",
              "      <th>Zip-code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>48067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>M</td>\n",
              "      <td>56</td>\n",
              "      <td>16</td>\n",
              "      <td>70072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>M</td>\n",
              "      <td>25</td>\n",
              "      <td>15</td>\n",
              "      <td>55117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>M</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>02460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>M</td>\n",
              "      <td>25</td>\n",
              "      <td>20</td>\n",
              "      <td>55455</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID Gender  Age  OccupationID Zip-code\n",
              "0       1      F    1            10    48067\n",
              "1       2      M   56            16    70072\n",
              "2       3      M   25            15    55117\n",
              "3       4      M   45             7    02460\n",
              "4       5      M   25            20    55455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX_NqRwmygFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1Ncz81hNfjBnCta53ntolILKvR-8YcMI0'\n",
        "fluff, id = link.split('=')\n",
        "file = drive.CreateFile({'id':id}) \n",
        "file.GetContentFile('movies.dat')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Xmp3Dj42dB",
        "colab_type": "code",
        "outputId": "5bf60182-2253-4817-e9d0-ea9d80323055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "movies_title = ['MovieID', 'Title', 'Genres']\n",
        "movies = pd.read_csv('movies.dat', sep='::', header=None, names=movies_title, engine='python')\n",
        "movies_orig = movies.values\n",
        "movies.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Animation|Children's|Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children's|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MovieID                               Title                        Genres\n",
              "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
              "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
              "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
              "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
              "4        5  Father of the Bride Part II (1995)                        Comedy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCCAY4Ey48LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=18BohWrf7VGldB53x9Z7HZPlDT11YAa5t'\n",
        "fluff, id = link.split('=')\n",
        "file = drive.CreateFile({'id':id}) \n",
        "file.GetContentFile('ratings.dat')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txd2MqLZ45Um",
        "colab_type": "code",
        "outputId": "bbda7cbd-b0b7-4687-9a3b-18eb9eeb847b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "ratings_title = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
        "ratings = pd.read_csv('ratings.dat', sep='::', header=None, names=ratings_title, engine='python')\n",
        "ratings.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID  MovieID  Rating  Timestamp\n",
              "0       1     1193       5  978300760\n",
              "1       1      661       3  978302109\n",
              "2       1      914       3  978301968\n",
              "3       1     3408       4  978300275\n",
              "4       1     2355       5  978824291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPVXXRw35LKV",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing:\n",
        "not change: userid, occupationid, movieid\n",
        "\n",
        "gender: F/M ~ 0/1\n",
        "\n",
        "age: convert to 0-7\n",
        "\n",
        "genres: vectors\n",
        "\n",
        "title: vectors\n",
        "\n",
        "drop: zip-code, timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRmmneX95II5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert gender\n",
        "gender_map = {'F':0, 'M':1}\n",
        "users.drop(['Zip-code'], axis=1,inplace=True)\n",
        "users['Gender'] = users['Gender'].map(gender_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkKDpPn_5U-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert age\n",
        "age_map = {val:i for i, val in enumerate(sorted(users['Age'].unique()))}\n",
        "users['Age'] = users['Age'].map(age_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSqiDitj5YUW",
        "colab_type": "code",
        "outputId": "f5e21da9-0c4d-43fb-a8be-599cca763417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "users.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>OccupationID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID  Gender  Age  OccupationID\n",
              "0       1       0    0            10\n",
              "1       2       1    6            16\n",
              "2       3       1    2            15\n",
              "3       4       1    4             7\n",
              "4       5       1    2            20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxGLhMaW5bJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove the year in the title\n",
        "pattern = re.compile(r'^(.*)\\((\\d+)\\)$') #r表示后面是一个正则表达式''\n",
        "                                        #^匹配开头,$匹配结尾,(.*)中的()表示匹配其中的任意正则表达式,.匹配任何字符,*代表可以重复0次或多次\n",
        "                                        #\\(和\\)：表示对括号的转义，匹配文本中真正的括号\n",
        "                                        #(\\d+)表示匹配()内的任意字符,\\d表示任何数字,+代表数字重复一次或者多次\n",
        "title_map = {val:pattern.match(val).group(1) for i, val in enumerate(movies['Title'].unique())}\n",
        "movies['Title'] = movies['Title'].map(title_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeyZIGtD5dz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert genres\n",
        "#将电影类型转成等长数字列表，长度是18\n",
        "genres_set = set()\n",
        "for val in movies['Genres'].str.split('|'):\n",
        "    genres_set.update(val) \n",
        "genres_set.add('<PAD>')\n",
        "genres_to_int = {val:i for i, val in enumerate(genres_set)}\n",
        "genres_map = {val:[genres_to_int[gen] for gen in val.split('|')] for i, val in enumerate(movies['Genres'].unique())}\n",
        "for key in genres_map:\n",
        "    for cnt in range(max(genres_to_int.values()) - len(genres_map[key])):\n",
        "        genres_map[key].insert(len(genres_map[key]) + cnt,genres_to_int['<PAD>'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhVBa1l05gfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies['Genres'] = movies['Genres'].map(genres_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loswUwZ85lHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert title\n",
        "title_set = set()\n",
        "for val in movies['Title'].str.split():\n",
        "    title_set.update(val) #添加新元素到集合当中，即完成出现电影中的新单词时，存下\n",
        "title_set.add('<PAD>')\n",
        "title_to_int = {val:i for i, val in enumerate(title_set)} #对单词像字典一样进行标注'描述电影的word：数字'格式,即数字字典\n",
        " \n",
        "title_count = 15\n",
        "title_map = {val:[title_to_int[word] for word in val.split()] for i, val in enumerate(movies['Title'].unique())}\n",
        "                                #val.split()得到全部被空格分开的电影名称字符串列表，row遍历电影集中一个电影的全部单词\n",
        "                                #title_map得到的是字典，格式为'一个电影字符串：[描述这个电影的全部单词构成的一个对应的数值列表]'\n",
        "\n",
        "for key in title_map:\n",
        "    for cnt in range(title_count - len(title_map[key])):\n",
        "        title_map[key].insert(len(title_map[key]) + cnt,title_to_int['<PAD>'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqvveP9C5n2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies['Title'] = movies['Title'].map(title_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPViIquQ5qZB",
        "colab_type": "code",
        "outputId": "5b517fe6-7da7-4fe3-b101-d022d4e9d761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "movies.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[1415, 3715, 2940, 2940, 2940, 2940, 2940, 294...</td>\n",
              "      <td>[11, 13, 8, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>[461, 2940, 2940, 2940, 2940, 2940, 2940, 2940...</td>\n",
              "      <td>[5, 13, 14, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>[5185, 1639, 1934, 2940, 2940, 2940, 2940, 294...</td>\n",
              "      <td>[8, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>[464, 4362, 792, 2940, 2940, 2940, 2940, 2940,...</td>\n",
              "      <td>[8, 15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>[2409, 2489, 3685, 521, 2888, 2597, 2940, 2940...</td>\n",
              "      <td>[8, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MovieID  ...                                             Genres\n",
              "0        1  ...  [11, 13, 8, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3...\n",
              "1        2  ...  [5, 13, 14, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3...\n",
              "2        3  ...  [8, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...\n",
              "3        4  ...  [8, 15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...\n",
              "4        5  ...  [8, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOP0gOJf5sRf",
        "colab_type": "code",
        "outputId": "a5b5705e-403e-45b5-def8-f89982e4bbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "ratings.drop('Timestamp', axis=1, inplace=True)\n",
        "ratings.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID  MovieID  Rating\n",
              "0       1     1193       5\n",
              "1       1      661       3\n",
              "2       1      914       3\n",
              "3       1     3408       4\n",
              "4       1     2355       5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNxQRh6c5vDQ",
        "colab_type": "code",
        "outputId": "895c8bbb-65be-477f-f019-b0a6973e5621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# combine data\n",
        "data = pd.merge(pd.merge(ratings, users), movies)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>OccupationID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>[4241, 4165, 3863, 3685, 3405, 4444, 2940, 294...</td>\n",
              "      <td>[15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>[4241, 4165, 3863, 3685, 3405, 4444, 2940, 294...</td>\n",
              "      <td>[15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>1193</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>[4241, 4165, 3863, 3685, 3405, 4444, 2940, 294...</td>\n",
              "      <td>[15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15</td>\n",
              "      <td>1193</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>[4241, 4165, 3863, 3685, 3405, 4444, 2940, 294...</td>\n",
              "      <td>[15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>[4241, 4165, 3863, 3685, 3405, 4444, 2940, 294...</td>\n",
              "      <td>[15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID  ...                                             Genres\n",
              "0       1  ...  [15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...\n",
              "1       2  ...  [15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...\n",
              "2      12  ...  [15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...\n",
              "3      15  ...  [15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...\n",
              "4      17  ...  [15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wfyZaEv50xM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_pd = data.drop(['Rating'], axis=1)\n",
        "targets_pd = data['Rating']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_GLs7U511D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = features_pd.values\n",
        "targets_values = targets_pd.values "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aFwV6zNnoIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    #title_count电影名长度15\n",
        "    #title_set {索引：去掉年份且不重复的电影名}\n",
        "    #genres_to_int {题材字符串列表：数字}\n",
        "    #features 去掉评分ratings列的三表合并信息，作为输入x。则列信息：userid,gender,age,occupation,movieid,title,genres\n",
        "    #targets_values 评分，学习目标y,三表合并后的对应ratings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rWqIIx6IGVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_params(params):\n",
        "    \"\"\"\n",
        "    Save parameters to file\n",
        "    \"\"\"\n",
        "    pickle.dump(params, open('params.p', 'wb'))\n",
        "\n",
        "\n",
        "def load_params():\n",
        "    \"\"\"\n",
        "    Load parameters from file\n",
        "    \"\"\"\n",
        "    return pickle.load(open('params.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKaCyA83Akv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#嵌入矩阵的维度\n",
        "embed_dim = 32\n",
        "#用户ID个数\n",
        "uid_max = max(features.take(0,1)) + 1 # 6040\n",
        "                                      #ndarray.take(indices, axis=None, out=None, mode='raise')从轴axis上获取数组中的元素，并以一维数组或者矩阵返回\n",
        "                                      #按axis选择处于indices位置上的值\n",
        "                                      #axis用于选择值的轴，0为横轴，1为纵向选\n",
        "                                      #如features.take(0,0)就会选择横向第一条数据，(1,0)会选择横向第二条数据\n",
        "#性别个数\n",
        "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
        "#年龄类别个数\n",
        "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
        "#职业个数\n",
        "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
        "\n",
        "#电影ID个数\n",
        "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
        "#电影类型个数\n",
        "movie_categories_max = max(genres_to_int.values()) + 1 # 18 + 1 = 19\n",
        "#电影名单词个数\n",
        "movie_title_max = len(title_set) # 5216\n",
        "\n",
        "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
        "combiner = \"sum\"\n",
        "\n",
        "#电影名长度\n",
        "sentences_size = title_count # = 15\n",
        "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
        "window_sizes = {2, 3, 4, 5}\n",
        "#文本卷积核数量\n",
        "filter_num = 8\n",
        "\n",
        "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
        "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUnHke6Tyd3k",
        "colab_type": "code",
        "outputId": "34433fee-443e-4973-84e9-9a4b816ea092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(uid_max, gender_max, age_max, job_max, movie_id_max, movie_categories_max, movie_title_max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6041 2 7 21 3953 19 5215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYkbq95H6A_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of Epochs\n",
        "num_epochs = 5\n",
        "# Batch Size\n",
        "batch_size = 256\n",
        "\n",
        "dropout_keep = 0.5\n",
        "# Learning Rate\n",
        "learning_rate = 0.0001\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 20\n",
        "\n",
        "save_dir = './save'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j-oZiUV6Fnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inputs():\n",
        "    #tf.placeholder(dtype, shape=None, name=None)此函数可以理解为形参，用于定义过程，在执行的时候再赋具体的值\n",
        "    #dtype：数据类型\n",
        "    #shape：数据形状。默认是None,[行数，列数]\n",
        "    #name：名称\n",
        "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")   # 这里一行代表一个用户的id，是batch×1，每一行是一个列表\n",
        "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
        "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
        "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
        "    \n",
        "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
        "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
        "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
        "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
        "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
        "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
        "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoNX582q6QP5",
        "colab_type": "text"
      },
      "source": [
        "# User features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffiIblXK6NSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#定义User的嵌入矩阵，完成原始矩阵经过嵌入层后得到的输出\n",
        "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
        "    with tf.name_scope(\"user_embedding\"): #用于后面tensorboard可视化图层关系\n",
        "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
        "        #tf.random_uniform(shape, minval=0,maxval=None,dtype=tf.float32) 从均匀分布中输出随机值。\n",
        "        #返回shape形状矩阵：用户数×特征数，产生于low(-1)和high(1)之间，产生的值是均匀分布的。\n",
        "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
        "        #tf.nn.embedding_lookup(tensor, id) 选取一个张量tensor里面索引id对应的元素\n",
        "        #选取uid_embed_matrix的用户id对应的某个用户id的向量, # uid 这里一行代表一个用户的id，是batch×1，每一行是一个列表\n",
        "    \n",
        "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\") \n",
        "                                                                          #这里特征数降一半\n",
        "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
        "                                                                          #选取gender_embed_matrix的用户性别对应的某个用户性别的向量\n",
        "\n",
        "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
        "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
        "        \n",
        "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
        "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
        "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBqMJgKe6Vh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#将User的嵌入矩阵一起全连接生成User的特征\n",
        "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
        "    with tf.name_scope(\"user_fc\"):\n",
        "        #第一层全连接\n",
        "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu) # 输入为batch*embed_dim\n",
        "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)  # 输入为batch*embed_dim//2\n",
        "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)   # 输入为batch*embed_dim//2\n",
        "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)    # 输入为batch*embed_dim//2\n",
        "\n",
        "        # tf.layers.dense(inputs,units,activation=None,use_bias=True,kernel_initializer=None,bias_initializer=tf.zeros_initializer(),\n",
        "                        # kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,\n",
        "                        # bias_constraint=None,trainable=True,name=None,reuse=None)\n",
        "                        #inputs:该层的输入; units:输出的大小(维数),整数或long; activation: 使用什么激活函数（神经网络的非线性层），默认为None，不使用激活函数\n",
        "                        #name该层的名字\n",
        "        \n",
        "        #第二层全连接\n",
        "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
        "                                                          #     对于三维来说0表示第一个括号维度，1表示第二个括号维度，2表示第三个括号维度\n",
        "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
        "    \n",
        "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])  #-1表示缺省值，满足其他维度要求，这里该是几就是几，(?, 200)\n",
        "    return user_combine_layer, user_combine_layer_flat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDIKwGA56a0M",
        "colab_type": "text"
      },
      "source": [
        "# movie features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9cfTz7L6Yg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_movie_id_embed_layer(movie_id):\n",
        "    with tf.name_scope(\"movie_embedding\"):\n",
        "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
        "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\") \n",
        "    return movie_id_embed_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAo0VzkY6dcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_movie_categories_layers(movie_categories):\n",
        "    with tf.name_scope(\"movie_categories_layers\"):\n",
        "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
        "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\") #(?,18,32)\n",
        "        if combiner == \"sum\":\n",
        "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True) #(?,1,32)\n",
        "                                                        #函数中的input_tensor是按照axis中已经给定的维度来减少的，axis表示按第几个维度求和\n",
        "                                                        #但是keep_dims为true，则维度不会减小\n",
        "                                                        #如果axis没有条目，则缩小所有维度，并返回具有单个元素的张量\n",
        "    #     elif combiner == \"mean\":\n",
        "\n",
        "    return movie_categories_embed_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj36Sg1c6gfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_movie_cnn_layer(movie_titles):\n",
        "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
        "    with tf.name_scope(\"movie_embedding\"):\n",
        "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
        "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
        "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)    #(?,15,32,1)\n",
        "                                          #tf.expand_dims(input, axis, name=None)在axis轴，维度增加一维，数值为1，axis=-1表示最后一位\n",
        "                                        #当然,我们常用tf.reshape(input, shape=[])可达到相同效果,但有时在构建图的过程中,placeholder没有被feed具体的值,就会报错\n",
        "                                        #'t' is a tensor of shape [2]\n",
        "                                        # shape(expand_dims(t, 0)) ==> [1, 2]\n",
        "                                        # shape(expand_dims(t, 1)) ==> [2, 1]\n",
        "                                        # shape(expand_dims(t, -1)) ==> [2, 1]\n",
        "    \n",
        "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
        "    pool_layer_lst = []\n",
        "    for window_size in window_sizes: #[2,3,4,5]\n",
        "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
        "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
        "                            #初始化卷积核参数\n",
        "                            #tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
        "                            #得到正态分布输出为shape，mean均值，stddev标准差\n",
        "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
        "                            #初始化bias\n",
        "                            #tf.constant(value,dtype=None,shape=None,name=’Const’) 创建一个常量tensor,按照给出value来赋值,可以用shape来指定其形状\n",
        "                            #这里的shape表示(8,)\n",
        "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
        "                          #tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)\n",
        "                          #input：指需要做卷积的输入图像 filter：相当于CNN中的卷积核 strides：卷积时在图像每一维的步长通常为[1,X,X,1]\n",
        "                          #padding：string类型的量，只能是\"SAME\",\"VALID\"其中之一,这里不填充\n",
        "                          #use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true\n",
        "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\") #最后迭代到windowsize=5时(?, 11, 1, 8)\n",
        "            \n",
        "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
        "                        #(?, 1, 1, 8)\n",
        "                        #tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
        "                        #value：需要池化的输入 \n",
        "                        #ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
        "                        #height=15-windowsize\n",
        "                        #strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
        "\n",
        "            #这里最后得到4个tensor堆叠的列表\n",
        "            pool_layer_lst.append(maxpool_layer)\n",
        "            #[<tf.Tensor 'movie_txt_conv_maxpool_2/maxpool_layer:0' shape=(?, 1, 1, 8) dtype=float32>, \n",
        "            #<tf.Tensor 'movie_txt_conv_maxpool_3/maxpool_layer:0' shape=(?, 1, 1, 8) dtype=float32>, \n",
        "            #<tf.Tensor 'movie_txt_conv_maxpool_4/maxpool_layer:0' shape=(?, 1, 1, 8) dtype=float32>, \n",
        "            #<tf.Tensor 'movie_txt_conv_maxpool_5/maxpool_layer:0' shape=(?, 1, 1, 8) dtype=float32>]\n",
        " \n",
        "\n",
        "    #Dropout层\n",
        "    with tf.name_scope(\"pool_dropout\"):\n",
        "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")   #(?, 1, 1, 32)\n",
        "        max_num = len(window_sizes) * filter_num\n",
        "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")  #(?, 1, 32)\n",
        "    \n",
        "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")  #(?, 1, 32)\n",
        "    return pool_layer_flat, dropout_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OXe6Xo_6mvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
        "    with tf.name_scope(\"movie_fc\"):\n",
        "        #第一层全连接\n",
        "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
        "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
        "    \n",
        "        #第二层全连接\n",
        "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
        "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
        "    \n",
        "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
        "    return movie_combine_layer, movie_combine_layer_flat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjRLpqlw6pz5",
        "colab_type": "code",
        "outputId": "7bcf58a6-3caf-44a8-ae22-3e3115b3f309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "tf.reset_default_graph() #用于清除默认图形堆栈并重置全局默认图形\n",
        "train_graph = tf.Graph()  # first creat a simple graph\n",
        "with train_graph.as_default():\n",
        "    #获取输入占位符\n",
        "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
        "    #获取User的4个嵌入向量\n",
        "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
        "    #得到用户特征\n",
        "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
        "    #获取电影ID的嵌入向量\n",
        "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
        "    #获取电影类型的嵌入向量\n",
        "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
        "    #获取电影名的特征向量\n",
        "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
        "    #得到电影特征\n",
        "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
        "                                                                                movie_categories_embed_layer, \n",
        "                                                                                dropout_layer)\n",
        "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
        "    with tf.name_scope(\"inference\"):\n",
        "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
        "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
        "#         inference = tf.layers.dense(inference_layer, 1,\n",
        "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
        "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
        "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
        "#        inference = tf.matmul(user_combine_layer_flat, tf.transpose(movie_combine_layer_flat))\n",
        "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)   #按axis=1求和降维，得(?,),*表示对应元素相乘\n",
        "        inference = tf.expand_dims(inference, axis=1)   #(batch,1)为了下面和target统一格式计算loss\n",
        "\n",
        "    with tf.name_scope(\"loss\"):\n",
        "        # MSE损失，将计算值回归到评分\n",
        "        cost = tf.losses.mean_squared_error(targets, inference )\n",
        "        loss = tf.reduce_mean(cost)\n",
        "    # 优化损失 \n",
        "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
        "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "    optimizer = tf.train.AdamOptimizer(lr)\n",
        "    gradients = optimizer.compute_gradients(loss)  #cost\n",
        "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-33-1fd9939200af>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-36-c664858e9427>:6: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From <ipython-input-38-0bd261b7cda7>:55: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn0yRNX16wfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(Xs, ys, batch_size):\n",
        "    for start in range(0, len(Xs), batch_size):\n",
        "        end = min(start + batch_size, len(Xs))\n",
        "        yield Xs[start:end], ys[start:end]\n",
        "        #yield 是一个类似 return 的关键字，迭代一次遇到yield时就返回yield后面(右边)的值。\n",
        "        #重点是：下一次迭代时，从上一次迭代遇到的yield后面的代码(下一行)开始执行。\n",
        "        #参见https://www.jianshu.com/p/d09778f4e055"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQopnmP_614e",
        "colab_type": "code",
        "outputId": "ce85f55c-e654-4023-e620-3dddf4d077b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "losses = {'train':[], 'test':[]}\n",
        "\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    \n",
        "    #搜集数据给tensorBoard用\n",
        "    # Keep track of gradient values and sparsity\n",
        "    grad_summaries = []\n",
        "    for g, v in gradients:\n",
        "        if g is not None:\n",
        "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
        "            #tf.summary.histogram('summary_name', tensor)用来显示直方图信息\n",
        "                                #将【计算图】中的【数据的分布/数据直方图】写入TensorFlow中的【日志文件】，以便为将来tensorboard的可视化做准备\n",
        "                                #一般用来显示训练过程中变量的分布情况\n",
        "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
        "            grad_summaries.append(grad_hist_summary)\n",
        "            grad_summaries.append(sparsity_summary)\n",
        "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "        \n",
        "    # Output directory for models and summaries\n",
        "    timestamp = str(int(time.time()))\n",
        "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "    print(\"Writing to {}\\n\".format(out_dir))\n",
        "     \n",
        "    # Summaries for loss and accuracy\n",
        "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "    # Train Summaries\n",
        "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
        "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "    # Inference summaries\n",
        "    inference_summary_op = tf.summary.merge([loss_summary])\n",
        "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
        "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()    #用于后面保存数据，创建一个saver对象\n",
        "    for epoch_i in range(num_epochs):\n",
        "        \n",
        "        #将数据集分成训练集和测试集，随机种子不固定\n",
        "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
        "                                                           targets_values,  \n",
        "                                                           test_size = 0.2,  \n",
        "                                                           random_state = 0)  \n",
        "        \n",
        "        train_batches = get_batches(train_X, train_y, batch_size)   #在分好的训练集中再选batch个\n",
        "        test_batches = get_batches(test_X, test_y, batch_size)\n",
        "    \n",
        "        #训练的迭代，保存训练损失\n",
        "        for batch_i in range(len(train_X) // batch_size):\n",
        "            x, y = next(train_batches)      #next() 返回迭代器的下一个项目，next(get_batches(train_X, train_y, batch_size))\n",
        "                                       #在这个for循环每次都进行上一个batch的后面一个batch\n",
        "\n",
        "            categories = np.zeros([batch_size, 18])\n",
        "            for i in range(batch_size):\n",
        "                categories[i] = x.take(6,1)[i]    #x取纵着的下标为6的全部数据\n",
        "\n",
        "            titles = np.zeros([batch_size, sentences_size])\n",
        "            for i in range(batch_size):\n",
        "                titles[i] = x.take(5,1)[i]\n",
        "\n",
        "            feed = {\n",
        "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
        "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
        "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
        "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
        "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
        "                movie_categories: categories,  #x.take(6,1)\n",
        "                movie_titles: titles,  #x.take(5,1)\n",
        "                targets: np.reshape(y, [batch_size, 1]),\n",
        "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
        "                lr: learning_rate}\n",
        "\n",
        "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
        "            losses['train'].append(train_loss)\n",
        "            train_summary_writer.add_summary(summaries, step)  #\n",
        "            \n",
        "            # Show every <show_every_n_batches> batches\n",
        "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
        "                    time_str,\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    (len(train_X) // batch_size),\n",
        "                    train_loss))\n",
        "                \n",
        "        #使用测试数据的迭代\n",
        "        for batch_i  in range(len(test_X) // batch_size):\n",
        "            x, y = next(test_batches)\n",
        "            \n",
        "            categories = np.zeros([batch_size, 18])\n",
        "            for i in range(batch_size):\n",
        "                categories[i] = x.take(6,1)[i]\n",
        "\n",
        "            titles = np.zeros([batch_size, sentences_size])\n",
        "            for i in range(batch_size):\n",
        "                titles[i] = x.take(5,1)[i]\n",
        "\n",
        "            feed = {\n",
        "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
        "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
        "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
        "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
        "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
        "                movie_categories: categories,  #x.take(6,1)\n",
        "                movie_titles: titles,  #x.take(5,1)\n",
        "                targets: np.reshape(y, [batch_size, 1]),\n",
        "                dropout_keep_prob: 1,\n",
        "                lr: learning_rate}\n",
        "            \n",
        "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
        "\n",
        "            #保存测试损失\n",
        "            losses['test'].append(test_loss)\n",
        "            inference_summary_writer.add_summary(summaries, step)  #\n",
        "\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
        "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
        "                    time_str,\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    (len(test_X) // batch_size),\n",
        "                    test_loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
        "    print('Model Trained and Saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing to /content/runs/1573183073\n",
            "\n",
            "2019-11-08T03:17:56.196179: Epoch   0 Batch    0/3125   train_loss = 14.594\n",
            "2019-11-08T03:17:57.279776: Epoch   0 Batch   20/3125   train_loss = 4.069\n",
            "2019-11-08T03:17:58.385722: Epoch   0 Batch   40/3125   train_loss = 2.737\n",
            "2019-11-08T03:17:59.432564: Epoch   0 Batch   60/3125   train_loss = 1.878\n",
            "2019-11-08T03:18:00.515457: Epoch   0 Batch   80/3125   train_loss = 2.029\n",
            "2019-11-08T03:18:01.611434: Epoch   0 Batch  100/3125   train_loss = 1.548\n",
            "2019-11-08T03:18:02.676651: Epoch   0 Batch  120/3125   train_loss = 1.663\n",
            "2019-11-08T03:18:03.804529: Epoch   0 Batch  140/3125   train_loss = 1.655\n",
            "2019-11-08T03:18:04.905007: Epoch   0 Batch  160/3125   train_loss = 1.344\n",
            "2019-11-08T03:18:05.995031: Epoch   0 Batch  180/3125   train_loss = 1.445\n",
            "2019-11-08T03:18:07.058908: Epoch   0 Batch  200/3125   train_loss = 1.774\n",
            "2019-11-08T03:18:08.140626: Epoch   0 Batch  220/3125   train_loss = 1.369\n",
            "2019-11-08T03:18:09.243971: Epoch   0 Batch  240/3125   train_loss = 1.449\n",
            "2019-11-08T03:18:10.342952: Epoch   0 Batch  260/3125   train_loss = 1.494\n",
            "2019-11-08T03:18:11.444638: Epoch   0 Batch  280/3125   train_loss = 1.459\n",
            "2019-11-08T03:18:12.572489: Epoch   0 Batch  300/3125   train_loss = 1.408\n",
            "2019-11-08T03:18:13.681449: Epoch   0 Batch  320/3125   train_loss = 1.377\n",
            "2019-11-08T03:18:14.760108: Epoch   0 Batch  340/3125   train_loss = 1.218\n",
            "2019-11-08T03:18:15.833923: Epoch   0 Batch  360/3125   train_loss = 1.431\n",
            "2019-11-08T03:18:16.945877: Epoch   0 Batch  380/3125   train_loss = 1.314\n",
            "2019-11-08T03:18:17.986495: Epoch   0 Batch  400/3125   train_loss = 1.221\n",
            "2019-11-08T03:18:19.054707: Epoch   0 Batch  420/3125   train_loss = 1.254\n",
            "2019-11-08T03:18:20.131414: Epoch   0 Batch  440/3125   train_loss = 1.384\n",
            "2019-11-08T03:18:21.176881: Epoch   0 Batch  460/3125   train_loss = 1.417\n",
            "2019-11-08T03:18:22.268538: Epoch   0 Batch  480/3125   train_loss = 1.298\n",
            "2019-11-08T03:18:23.368502: Epoch   0 Batch  500/3125   train_loss = 0.996\n",
            "2019-11-08T03:18:24.444961: Epoch   0 Batch  520/3125   train_loss = 1.266\n",
            "2019-11-08T03:18:25.509515: Epoch   0 Batch  540/3125   train_loss = 1.277\n",
            "2019-11-08T03:18:26.605839: Epoch   0 Batch  560/3125   train_loss = 1.430\n",
            "2019-11-08T03:18:27.687061: Epoch   0 Batch  580/3125   train_loss = 1.406\n",
            "2019-11-08T03:18:28.828609: Epoch   0 Batch  600/3125   train_loss = 1.360\n",
            "2019-11-08T03:18:29.914876: Epoch   0 Batch  620/3125   train_loss = 1.342\n",
            "2019-11-08T03:18:31.004401: Epoch   0 Batch  640/3125   train_loss = 1.302\n",
            "2019-11-08T03:18:32.076537: Epoch   0 Batch  660/3125   train_loss = 1.257\n",
            "2019-11-08T03:18:33.145073: Epoch   0 Batch  680/3125   train_loss = 1.172\n",
            "2019-11-08T03:18:34.229338: Epoch   0 Batch  700/3125   train_loss = 1.318\n",
            "2019-11-08T03:18:35.329694: Epoch   0 Batch  720/3125   train_loss = 1.191\n",
            "2019-11-08T03:18:36.381003: Epoch   0 Batch  740/3125   train_loss = 1.252\n",
            "2019-11-08T03:18:37.462614: Epoch   0 Batch  760/3125   train_loss = 1.308\n",
            "2019-11-08T03:18:38.527672: Epoch   0 Batch  780/3125   train_loss = 1.367\n",
            "2019-11-08T03:18:39.637427: Epoch   0 Batch  800/3125   train_loss = 1.295\n",
            "2019-11-08T03:18:40.718739: Epoch   0 Batch  820/3125   train_loss = 1.215\n",
            "2019-11-08T03:18:41.783798: Epoch   0 Batch  840/3125   train_loss = 1.227\n",
            "2019-11-08T03:18:42.839655: Epoch   0 Batch  860/3125   train_loss = 1.197\n",
            "2019-11-08T03:18:43.914048: Epoch   0 Batch  880/3125   train_loss = 1.246\n",
            "2019-11-08T03:18:45.009097: Epoch   0 Batch  900/3125   train_loss = 1.182\n",
            "2019-11-08T03:18:46.057681: Epoch   0 Batch  920/3125   train_loss = 1.346\n",
            "2019-11-08T03:18:47.078485: Epoch   0 Batch  940/3125   train_loss = 1.331\n",
            "2019-11-08T03:18:48.130603: Epoch   0 Batch  960/3125   train_loss = 1.337\n",
            "2019-11-08T03:18:49.201249: Epoch   0 Batch  980/3125   train_loss = 1.367\n",
            "2019-11-08T03:18:50.240899: Epoch   0 Batch 1000/3125   train_loss = 1.292\n",
            "2019-11-08T03:18:51.357065: Epoch   0 Batch 1020/3125   train_loss = 1.348\n",
            "2019-11-08T03:18:52.384115: Epoch   0 Batch 1040/3125   train_loss = 1.218\n",
            "2019-11-08T03:18:53.452624: Epoch   0 Batch 1060/3125   train_loss = 1.461\n",
            "2019-11-08T03:18:54.509662: Epoch   0 Batch 1080/3125   train_loss = 1.215\n",
            "2019-11-08T03:18:55.582057: Epoch   0 Batch 1100/3125   train_loss = 1.283\n",
            "2019-11-08T03:18:56.643628: Epoch   0 Batch 1120/3125   train_loss = 1.231\n",
            "2019-11-08T03:18:57.688864: Epoch   0 Batch 1140/3125   train_loss = 1.253\n",
            "2019-11-08T03:18:58.783013: Epoch   0 Batch 1160/3125   train_loss = 1.239\n",
            "2019-11-08T03:18:59.882466: Epoch   0 Batch 1180/3125   train_loss = 1.280\n",
            "2019-11-08T03:19:00.938507: Epoch   0 Batch 1200/3125   train_loss = 1.236\n",
            "2019-11-08T03:19:02.046252: Epoch   0 Batch 1220/3125   train_loss = 1.091\n",
            "2019-11-08T03:19:03.122389: Epoch   0 Batch 1240/3125   train_loss = 1.124\n",
            "2019-11-08T03:19:04.223791: Epoch   0 Batch 1260/3125   train_loss = 1.231\n",
            "2019-11-08T03:19:05.306579: Epoch   0 Batch 1280/3125   train_loss = 1.160\n",
            "2019-11-08T03:19:06.403667: Epoch   0 Batch 1300/3125   train_loss = 1.208\n",
            "2019-11-08T03:19:07.467247: Epoch   0 Batch 1320/3125   train_loss = 1.222\n",
            "2019-11-08T03:19:08.565213: Epoch   0 Batch 1340/3125   train_loss = 1.103\n",
            "2019-11-08T03:19:09.689042: Epoch   0 Batch 1360/3125   train_loss = 1.152\n",
            "2019-11-08T03:19:10.766270: Epoch   0 Batch 1380/3125   train_loss = 1.076\n",
            "2019-11-08T03:19:11.901763: Epoch   0 Batch 1400/3125   train_loss = 1.242\n",
            "2019-11-08T03:19:13.007216: Epoch   0 Batch 1420/3125   train_loss = 1.248\n",
            "2019-11-08T03:19:14.074616: Epoch   0 Batch 1440/3125   train_loss = 1.160\n",
            "2019-11-08T03:19:15.164738: Epoch   0 Batch 1460/3125   train_loss = 1.211\n",
            "2019-11-08T03:19:16.241771: Epoch   0 Batch 1480/3125   train_loss = 1.202\n",
            "2019-11-08T03:19:17.306861: Epoch   0 Batch 1500/3125   train_loss = 1.328\n",
            "2019-11-08T03:19:18.382991: Epoch   0 Batch 1520/3125   train_loss = 1.272\n",
            "2019-11-08T03:19:19.500295: Epoch   0 Batch 1540/3125   train_loss = 1.236\n",
            "2019-11-08T03:19:20.644502: Epoch   0 Batch 1560/3125   train_loss = 1.202\n",
            "2019-11-08T03:19:21.698317: Epoch   0 Batch 1580/3125   train_loss = 1.193\n",
            "2019-11-08T03:19:22.766051: Epoch   0 Batch 1600/3125   train_loss = 1.253\n",
            "2019-11-08T03:19:23.838500: Epoch   0 Batch 1620/3125   train_loss = 1.176\n",
            "2019-11-08T03:19:24.950069: Epoch   0 Batch 1640/3125   train_loss = 1.289\n",
            "2019-11-08T03:19:26.061770: Epoch   0 Batch 1660/3125   train_loss = 1.272\n",
            "2019-11-08T03:19:27.155232: Epoch   0 Batch 1680/3125   train_loss = 1.223\n",
            "2019-11-08T03:19:28.264480: Epoch   0 Batch 1700/3125   train_loss = 1.034\n",
            "2019-11-08T03:19:29.348485: Epoch   0 Batch 1720/3125   train_loss = 1.153\n",
            "2019-11-08T03:19:30.494976: Epoch   0 Batch 1740/3125   train_loss = 1.134\n",
            "2019-11-08T03:19:31.596864: Epoch   0 Batch 1760/3125   train_loss = 1.290\n",
            "2019-11-08T03:19:32.666256: Epoch   0 Batch 1780/3125   train_loss = 1.092\n",
            "2019-11-08T03:19:33.817285: Epoch   0 Batch 1800/3125   train_loss = 1.178\n",
            "2019-11-08T03:19:34.961251: Epoch   0 Batch 1820/3125   train_loss = 1.238\n",
            "2019-11-08T03:19:36.086283: Epoch   0 Batch 1840/3125   train_loss = 1.242\n",
            "2019-11-08T03:19:37.144921: Epoch   0 Batch 1860/3125   train_loss = 1.198\n",
            "2019-11-08T03:19:38.268309: Epoch   0 Batch 1880/3125   train_loss = 1.274\n",
            "2019-11-08T03:19:39.381008: Epoch   0 Batch 1900/3125   train_loss = 1.078\n",
            "2019-11-08T03:19:40.502305: Epoch   0 Batch 1920/3125   train_loss = 1.114\n",
            "2019-11-08T03:19:41.664580: Epoch   0 Batch 1940/3125   train_loss = 1.070\n",
            "2019-11-08T03:19:42.826264: Epoch   0 Batch 1960/3125   train_loss = 1.123\n",
            "2019-11-08T03:19:43.936142: Epoch   0 Batch 1980/3125   train_loss = 1.174\n",
            "2019-11-08T03:19:45.067442: Epoch   0 Batch 2000/3125   train_loss = 1.405\n",
            "2019-11-08T03:19:46.155932: Epoch   0 Batch 2020/3125   train_loss = 1.296\n",
            "2019-11-08T03:19:47.268941: Epoch   0 Batch 2040/3125   train_loss = 1.131\n",
            "2019-11-08T03:19:48.394286: Epoch   0 Batch 2060/3125   train_loss = 1.006\n",
            "2019-11-08T03:19:49.501092: Epoch   0 Batch 2080/3125   train_loss = 1.290\n",
            "2019-11-08T03:19:50.612728: Epoch   0 Batch 2100/3125   train_loss = 1.132\n",
            "2019-11-08T03:19:51.740320: Epoch   0 Batch 2120/3125   train_loss = 1.106\n",
            "2019-11-08T03:19:52.854754: Epoch   0 Batch 2140/3125   train_loss = 1.193\n",
            "2019-11-08T03:19:54.003111: Epoch   0 Batch 2160/3125   train_loss = 1.160\n",
            "2019-11-08T03:19:55.086145: Epoch   0 Batch 2180/3125   train_loss = 1.173\n",
            "2019-11-08T03:19:56.267639: Epoch   0 Batch 2200/3125   train_loss = 1.094\n",
            "2019-11-08T03:19:57.383858: Epoch   0 Batch 2220/3125   train_loss = 1.120\n",
            "2019-11-08T03:19:58.511882: Epoch   0 Batch 2240/3125   train_loss = 0.956\n",
            "2019-11-08T03:19:59.587356: Epoch   0 Batch 2260/3125   train_loss = 1.057\n",
            "2019-11-08T03:20:00.674124: Epoch   0 Batch 2280/3125   train_loss = 1.186\n",
            "2019-11-08T03:20:01.803005: Epoch   0 Batch 2300/3125   train_loss = 1.190\n",
            "2019-11-08T03:20:02.928846: Epoch   0 Batch 2320/3125   train_loss = 1.292\n",
            "2019-11-08T03:20:04.028634: Epoch   0 Batch 2340/3125   train_loss = 1.171\n",
            "2019-11-08T03:20:05.087962: Epoch   0 Batch 2360/3125   train_loss = 1.157\n",
            "2019-11-08T03:20:06.197899: Epoch   0 Batch 2380/3125   train_loss = 1.161\n",
            "2019-11-08T03:20:07.273182: Epoch   0 Batch 2400/3125   train_loss = 1.267\n",
            "2019-11-08T03:20:08.355623: Epoch   0 Batch 2420/3125   train_loss = 1.145\n",
            "2019-11-08T03:20:09.451747: Epoch   0 Batch 2440/3125   train_loss = 1.269\n",
            "2019-11-08T03:20:10.559849: Epoch   0 Batch 2460/3125   train_loss = 1.105\n",
            "2019-11-08T03:20:11.624227: Epoch   0 Batch 2480/3125   train_loss = 1.227\n",
            "2019-11-08T03:20:12.722922: Epoch   0 Batch 2500/3125   train_loss = 1.175\n",
            "2019-11-08T03:20:13.807105: Epoch   0 Batch 2520/3125   train_loss = 1.138\n",
            "2019-11-08T03:20:14.859504: Epoch   0 Batch 2540/3125   train_loss = 1.101\n",
            "2019-11-08T03:20:15.991703: Epoch   0 Batch 2560/3125   train_loss = 0.945\n",
            "2019-11-08T03:20:17.069498: Epoch   0 Batch 2580/3125   train_loss = 1.074\n",
            "2019-11-08T03:20:18.121668: Epoch   0 Batch 2600/3125   train_loss = 1.134\n",
            "2019-11-08T03:20:19.182039: Epoch   0 Batch 2620/3125   train_loss = 1.019\n",
            "2019-11-08T03:20:20.245581: Epoch   0 Batch 2640/3125   train_loss = 1.123\n",
            "2019-11-08T03:20:21.367689: Epoch   0 Batch 2660/3125   train_loss = 1.196\n",
            "2019-11-08T03:20:22.461230: Epoch   0 Batch 2680/3125   train_loss = 1.069\n",
            "2019-11-08T03:20:23.527108: Epoch   0 Batch 2700/3125   train_loss = 1.167\n",
            "2019-11-08T03:20:24.596756: Epoch   0 Batch 2720/3125   train_loss = 1.140\n",
            "2019-11-08T03:20:25.667108: Epoch   0 Batch 2740/3125   train_loss = 1.151\n",
            "2019-11-08T03:20:26.787435: Epoch   0 Batch 2760/3125   train_loss = 1.213\n",
            "2019-11-08T03:20:27.935050: Epoch   0 Batch 2780/3125   train_loss = 1.152\n",
            "2019-11-08T03:20:29.033828: Epoch   0 Batch 2800/3125   train_loss = 1.336\n",
            "2019-11-08T03:20:30.076685: Epoch   0 Batch 2820/3125   train_loss = 1.364\n",
            "2019-11-08T03:20:31.162918: Epoch   0 Batch 2840/3125   train_loss = 1.175\n",
            "2019-11-08T03:20:32.222741: Epoch   0 Batch 2860/3125   train_loss = 1.107\n",
            "2019-11-08T03:20:33.281021: Epoch   0 Batch 2880/3125   train_loss = 1.175\n",
            "2019-11-08T03:20:34.349497: Epoch   0 Batch 2900/3125   train_loss = 1.134\n",
            "2019-11-08T03:20:35.365373: Epoch   0 Batch 2920/3125   train_loss = 1.147\n",
            "2019-11-08T03:20:36.404542: Epoch   0 Batch 2940/3125   train_loss = 1.154\n",
            "2019-11-08T03:20:37.458750: Epoch   0 Batch 2960/3125   train_loss = 1.198\n",
            "2019-11-08T03:20:38.563316: Epoch   0 Batch 2980/3125   train_loss = 1.151\n",
            "2019-11-08T03:20:39.610984: Epoch   0 Batch 3000/3125   train_loss = 1.148\n",
            "2019-11-08T03:20:40.702478: Epoch   0 Batch 3020/3125   train_loss = 1.208\n",
            "2019-11-08T03:20:41.741719: Epoch   0 Batch 3040/3125   train_loss = 1.085\n",
            "2019-11-08T03:20:42.818949: Epoch   0 Batch 3060/3125   train_loss = 1.164\n",
            "2019-11-08T03:20:43.909230: Epoch   0 Batch 3080/3125   train_loss = 1.225\n",
            "2019-11-08T03:20:44.975295: Epoch   0 Batch 3100/3125   train_loss = 1.177\n",
            "2019-11-08T03:20:45.991920: Epoch   0 Batch 3120/3125   train_loss = 1.013\n",
            "2019-11-08T03:20:46.312195: Epoch   0 Batch    0/781   test_loss = 1.002\n",
            "2019-11-08T03:20:46.554907: Epoch   0 Batch   20/781   test_loss = 1.157\n",
            "2019-11-08T03:20:46.790345: Epoch   0 Batch   40/781   test_loss = 1.135\n",
            "2019-11-08T03:20:47.044692: Epoch   0 Batch   60/781   test_loss = 1.302\n",
            "2019-11-08T03:20:47.285058: Epoch   0 Batch   80/781   test_loss = 1.333\n",
            "2019-11-08T03:20:47.545623: Epoch   0 Batch  100/781   test_loss = 1.389\n",
            "2019-11-08T03:20:47.790870: Epoch   0 Batch  120/781   test_loss = 1.250\n",
            "2019-11-08T03:20:48.041106: Epoch   0 Batch  140/781   test_loss = 1.188\n",
            "2019-11-08T03:20:48.298237: Epoch   0 Batch  160/781   test_loss = 1.369\n",
            "2019-11-08T03:20:48.542932: Epoch   0 Batch  180/781   test_loss = 1.290\n",
            "2019-11-08T03:20:48.784823: Epoch   0 Batch  200/781   test_loss = 1.211\n",
            "2019-11-08T03:20:49.072670: Epoch   0 Batch  220/781   test_loss = 0.981\n",
            "2019-11-08T03:20:49.328985: Epoch   0 Batch  240/781   test_loss = 1.158\n",
            "2019-11-08T03:20:49.582187: Epoch   0 Batch  260/781   test_loss = 1.132\n",
            "2019-11-08T03:20:49.825259: Epoch   0 Batch  280/781   test_loss = 1.460\n",
            "2019-11-08T03:20:50.076071: Epoch   0 Batch  300/781   test_loss = 1.198\n",
            "2019-11-08T03:20:50.307685: Epoch   0 Batch  320/781   test_loss = 1.313\n",
            "2019-11-08T03:20:50.573681: Epoch   0 Batch  340/781   test_loss = 0.870\n",
            "2019-11-08T03:20:50.839753: Epoch   0 Batch  360/781   test_loss = 1.220\n",
            "2019-11-08T03:20:51.073292: Epoch   0 Batch  380/781   test_loss = 1.117\n",
            "2019-11-08T03:20:51.327941: Epoch   0 Batch  400/781   test_loss = 1.154\n",
            "2019-11-08T03:20:51.589954: Epoch   0 Batch  420/781   test_loss = 1.037\n",
            "2019-11-08T03:20:51.844185: Epoch   0 Batch  440/781   test_loss = 1.228\n",
            "2019-11-08T03:20:52.096511: Epoch   0 Batch  460/781   test_loss = 1.111\n",
            "2019-11-08T03:20:52.348665: Epoch   0 Batch  480/781   test_loss = 1.058\n",
            "2019-11-08T03:20:52.608571: Epoch   0 Batch  500/781   test_loss = 1.002\n",
            "2019-11-08T03:20:52.856351: Epoch   0 Batch  520/781   test_loss = 1.201\n",
            "2019-11-08T03:20:53.104391: Epoch   0 Batch  540/781   test_loss = 1.062\n",
            "2019-11-08T03:20:53.388452: Epoch   0 Batch  560/781   test_loss = 1.247\n",
            "2019-11-08T03:20:53.658361: Epoch   0 Batch  580/781   test_loss = 1.106\n",
            "2019-11-08T03:20:53.923606: Epoch   0 Batch  600/781   test_loss = 1.185\n",
            "2019-11-08T03:20:54.178291: Epoch   0 Batch  620/781   test_loss = 1.181\n",
            "2019-11-08T03:20:54.450095: Epoch   0 Batch  640/781   test_loss = 1.191\n",
            "2019-11-08T03:20:54.713970: Epoch   0 Batch  660/781   test_loss = 1.149\n",
            "2019-11-08T03:20:54.965521: Epoch   0 Batch  680/781   test_loss = 1.382\n",
            "2019-11-08T03:20:55.220557: Epoch   0 Batch  700/781   test_loss = 1.144\n",
            "2019-11-08T03:20:55.498065: Epoch   0 Batch  720/781   test_loss = 1.331\n",
            "2019-11-08T03:20:55.788859: Epoch   0 Batch  740/781   test_loss = 1.159\n",
            "2019-11-08T03:20:56.062545: Epoch   0 Batch  760/781   test_loss = 1.184\n",
            "2019-11-08T03:20:56.318790: Epoch   0 Batch  780/781   test_loss = 1.202\n",
            "2019-11-08T03:20:57.814053: Epoch   1 Batch   15/3125   train_loss = 1.204\n",
            "2019-11-08T03:20:58.879614: Epoch   1 Batch   35/3125   train_loss = 1.094\n",
            "2019-11-08T03:20:59.917949: Epoch   1 Batch   55/3125   train_loss = 1.239\n",
            "2019-11-08T03:21:00.936812: Epoch   1 Batch   75/3125   train_loss = 1.067\n",
            "2019-11-08T03:21:01.949614: Epoch   1 Batch   95/3125   train_loss = 0.981\n",
            "2019-11-08T03:21:02.985886: Epoch   1 Batch  115/3125   train_loss = 1.192\n",
            "2019-11-08T03:21:04.005197: Epoch   1 Batch  135/3125   train_loss = 0.999\n",
            "2019-11-08T03:21:05.061517: Epoch   1 Batch  155/3125   train_loss = 1.042\n",
            "2019-11-08T03:21:06.129176: Epoch   1 Batch  175/3125   train_loss = 1.086\n",
            "2019-11-08T03:21:07.182972: Epoch   1 Batch  195/3125   train_loss = 1.128\n",
            "2019-11-08T03:21:08.277749: Epoch   1 Batch  215/3125   train_loss = 1.101\n",
            "2019-11-08T03:21:09.393443: Epoch   1 Batch  235/3125   train_loss = 1.060\n",
            "2019-11-08T03:21:10.533277: Epoch   1 Batch  255/3125   train_loss = 1.209\n",
            "2019-11-08T03:21:11.611947: Epoch   1 Batch  275/3125   train_loss = 1.035\n",
            "2019-11-08T03:21:12.718046: Epoch   1 Batch  295/3125   train_loss = 1.008\n",
            "2019-11-08T03:21:13.758023: Epoch   1 Batch  315/3125   train_loss = 1.110\n",
            "2019-11-08T03:21:14.807652: Epoch   1 Batch  335/3125   train_loss = 0.997\n",
            "2019-11-08T03:21:15.840485: Epoch   1 Batch  355/3125   train_loss = 1.083\n",
            "2019-11-08T03:21:16.858327: Epoch   1 Batch  375/3125   train_loss = 1.143\n",
            "2019-11-08T03:21:17.858674: Epoch   1 Batch  395/3125   train_loss = 1.038\n",
            "2019-11-08T03:21:18.946101: Epoch   1 Batch  415/3125   train_loss = 1.267\n",
            "2019-11-08T03:21:20.050540: Epoch   1 Batch  435/3125   train_loss = 1.132\n",
            "2019-11-08T03:21:21.134700: Epoch   1 Batch  455/3125   train_loss = 1.024\n",
            "2019-11-08T03:21:22.169863: Epoch   1 Batch  475/3125   train_loss = 1.148\n",
            "2019-11-08T03:21:23.207647: Epoch   1 Batch  495/3125   train_loss = 1.089\n",
            "2019-11-08T03:21:24.238251: Epoch   1 Batch  515/3125   train_loss = 1.126\n",
            "2019-11-08T03:21:25.296833: Epoch   1 Batch  535/3125   train_loss = 1.164\n",
            "2019-11-08T03:21:26.357616: Epoch   1 Batch  555/3125   train_loss = 1.251\n",
            "2019-11-08T03:21:27.411560: Epoch   1 Batch  575/3125   train_loss = 1.114\n",
            "2019-11-08T03:21:28.453278: Epoch   1 Batch  595/3125   train_loss = 1.264\n",
            "2019-11-08T03:21:29.499116: Epoch   1 Batch  615/3125   train_loss = 1.023\n",
            "2019-11-08T03:21:30.492539: Epoch   1 Batch  635/3125   train_loss = 1.123\n",
            "2019-11-08T03:21:31.494997: Epoch   1 Batch  655/3125   train_loss = 1.040\n",
            "2019-11-08T03:21:32.541972: Epoch   1 Batch  675/3125   train_loss = 0.907\n",
            "2019-11-08T03:21:33.608590: Epoch   1 Batch  695/3125   train_loss = 1.049\n",
            "2019-11-08T03:21:34.652659: Epoch   1 Batch  715/3125   train_loss = 1.129\n",
            "2019-11-08T03:21:35.688695: Epoch   1 Batch  735/3125   train_loss = 0.960\n",
            "2019-11-08T03:21:36.714994: Epoch   1 Batch  755/3125   train_loss = 1.159\n",
            "2019-11-08T03:21:37.740413: Epoch   1 Batch  775/3125   train_loss = 1.042\n",
            "2019-11-08T03:21:38.787948: Epoch   1 Batch  795/3125   train_loss = 1.146\n",
            "2019-11-08T03:21:39.843455: Epoch   1 Batch  815/3125   train_loss = 1.156\n",
            "2019-11-08T03:21:40.977782: Epoch   1 Batch  835/3125   train_loss = 1.047\n",
            "2019-11-08T03:21:42.001067: Epoch   1 Batch  855/3125   train_loss = 1.268\n",
            "2019-11-08T03:21:43.054278: Epoch   1 Batch  875/3125   train_loss = 1.171\n",
            "2019-11-08T03:21:44.114706: Epoch   1 Batch  895/3125   train_loss = 0.972\n",
            "2019-11-08T03:21:45.142244: Epoch   1 Batch  915/3125   train_loss = 1.066\n",
            "2019-11-08T03:21:46.191014: Epoch   1 Batch  935/3125   train_loss = 1.151\n",
            "2019-11-08T03:21:47.209525: Epoch   1 Batch  955/3125   train_loss = 1.150\n",
            "2019-11-08T03:21:48.214776: Epoch   1 Batch  975/3125   train_loss = 1.136\n",
            "2019-11-08T03:21:49.245357: Epoch   1 Batch  995/3125   train_loss = 0.925\n",
            "2019-11-08T03:21:50.285802: Epoch   1 Batch 1015/3125   train_loss = 1.114\n",
            "2019-11-08T03:21:51.423806: Epoch   1 Batch 1035/3125   train_loss = 1.070\n",
            "2019-11-08T03:21:52.524229: Epoch   1 Batch 1055/3125   train_loss = 1.069\n",
            "2019-11-08T03:21:53.610798: Epoch   1 Batch 1075/3125   train_loss = 1.079\n",
            "2019-11-08T03:21:54.718723: Epoch   1 Batch 1095/3125   train_loss = 1.040\n",
            "2019-11-08T03:21:55.867975: Epoch   1 Batch 1115/3125   train_loss = 1.124\n",
            "2019-11-08T03:21:56.958188: Epoch   1 Batch 1135/3125   train_loss = 1.008\n",
            "2019-11-08T03:21:58.049415: Epoch   1 Batch 1155/3125   train_loss = 1.157\n",
            "2019-11-08T03:21:59.173469: Epoch   1 Batch 1175/3125   train_loss = 1.134\n",
            "2019-11-08T03:22:00.262547: Epoch   1 Batch 1195/3125   train_loss = 1.246\n",
            "2019-11-08T03:22:01.322079: Epoch   1 Batch 1215/3125   train_loss = 1.059\n",
            "2019-11-08T03:22:02.406671: Epoch   1 Batch 1235/3125   train_loss = 1.135\n",
            "2019-11-08T03:22:03.521561: Epoch   1 Batch 1255/3125   train_loss = 1.032\n",
            "2019-11-08T03:22:04.590874: Epoch   1 Batch 1275/3125   train_loss = 1.060\n",
            "2019-11-08T03:22:05.677169: Epoch   1 Batch 1295/3125   train_loss = 1.101\n",
            "2019-11-08T03:22:06.725528: Epoch   1 Batch 1315/3125   train_loss = 1.190\n",
            "2019-11-08T03:22:07.773974: Epoch   1 Batch 1335/3125   train_loss = 1.035\n",
            "2019-11-08T03:22:08.815235: Epoch   1 Batch 1355/3125   train_loss = 1.039\n",
            "2019-11-08T03:22:09.884210: Epoch   1 Batch 1375/3125   train_loss = 1.164\n",
            "2019-11-08T03:22:10.949860: Epoch   1 Batch 1395/3125   train_loss = 1.023\n",
            "2019-11-08T03:22:12.043676: Epoch   1 Batch 1415/3125   train_loss = 1.124\n",
            "2019-11-08T03:22:13.128908: Epoch   1 Batch 1435/3125   train_loss = 1.187\n",
            "2019-11-08T03:22:14.219545: Epoch   1 Batch 1455/3125   train_loss = 1.197\n",
            "2019-11-08T03:22:15.317635: Epoch   1 Batch 1475/3125   train_loss = 1.129\n",
            "2019-11-08T03:22:16.423898: Epoch   1 Batch 1495/3125   train_loss = 1.062\n",
            "2019-11-08T03:22:17.505731: Epoch   1 Batch 1515/3125   train_loss = 0.926\n",
            "2019-11-08T03:22:18.558411: Epoch   1 Batch 1535/3125   train_loss = 0.838\n",
            "2019-11-08T03:22:19.657528: Epoch   1 Batch 1555/3125   train_loss = 1.037\n",
            "2019-11-08T03:22:20.731364: Epoch   1 Batch 1575/3125   train_loss = 1.053\n",
            "2019-11-08T03:22:21.751648: Epoch   1 Batch 1595/3125   train_loss = 1.166\n",
            "2019-11-08T03:22:22.793647: Epoch   1 Batch 1615/3125   train_loss = 1.054\n",
            "2019-11-08T03:22:23.832798: Epoch   1 Batch 1635/3125   train_loss = 1.101\n",
            "2019-11-08T03:22:24.896003: Epoch   1 Batch 1655/3125   train_loss = 1.091\n",
            "2019-11-08T03:22:25.981235: Epoch   1 Batch 1675/3125   train_loss = 1.038\n",
            "2019-11-08T03:22:27.029243: Epoch   1 Batch 1695/3125   train_loss = 1.075\n",
            "2019-11-08T03:22:28.029209: Epoch   1 Batch 1715/3125   train_loss = 0.970\n",
            "2019-11-08T03:22:29.091883: Epoch   1 Batch 1735/3125   train_loss = 1.195\n",
            "2019-11-08T03:22:30.123454: Epoch   1 Batch 1755/3125   train_loss = 1.013\n",
            "2019-11-08T03:22:31.179642: Epoch   1 Batch 1775/3125   train_loss = 1.069\n",
            "2019-11-08T03:22:32.226868: Epoch   1 Batch 1795/3125   train_loss = 1.098\n",
            "2019-11-08T03:22:33.237579: Epoch   1 Batch 1815/3125   train_loss = 1.013\n",
            "2019-11-08T03:22:34.267626: Epoch   1 Batch 1835/3125   train_loss = 1.095\n",
            "2019-11-08T03:22:35.349310: Epoch   1 Batch 1855/3125   train_loss = 0.973\n",
            "2019-11-08T03:22:36.385029: Epoch   1 Batch 1875/3125   train_loss = 1.108\n",
            "2019-11-08T03:22:37.400341: Epoch   1 Batch 1895/3125   train_loss = 1.022\n",
            "2019-11-08T03:22:38.441524: Epoch   1 Batch 1915/3125   train_loss = 0.935\n",
            "2019-11-08T03:22:39.505354: Epoch   1 Batch 1935/3125   train_loss = 0.993\n",
            "2019-11-08T03:22:40.609721: Epoch   1 Batch 1955/3125   train_loss = 1.027\n",
            "2019-11-08T03:22:41.695283: Epoch   1 Batch 1975/3125   train_loss = 1.031\n",
            "2019-11-08T03:22:42.791797: Epoch   1 Batch 1995/3125   train_loss = 1.050\n",
            "2019-11-08T03:22:43.849934: Epoch   1 Batch 2015/3125   train_loss = 1.118\n",
            "2019-11-08T03:22:44.917044: Epoch   1 Batch 2035/3125   train_loss = 1.168\n",
            "2019-11-08T03:22:45.959942: Epoch   1 Batch 2055/3125   train_loss = 0.986\n",
            "2019-11-08T03:22:46.985061: Epoch   1 Batch 2075/3125   train_loss = 1.169\n",
            "2019-11-08T03:22:48.003127: Epoch   1 Batch 2095/3125   train_loss = 1.014\n",
            "2019-11-08T03:22:49.070895: Epoch   1 Batch 2115/3125   train_loss = 1.242\n",
            "2019-11-08T03:22:50.149915: Epoch   1 Batch 2135/3125   train_loss = 1.004\n",
            "2019-11-08T03:22:51.219185: Epoch   1 Batch 2155/3125   train_loss = 1.001\n",
            "2019-11-08T03:22:52.287652: Epoch   1 Batch 2175/3125   train_loss = 0.986\n",
            "2019-11-08T03:22:53.308770: Epoch   1 Batch 2195/3125   train_loss = 1.005\n",
            "2019-11-08T03:22:54.356077: Epoch   1 Batch 2215/3125   train_loss = 1.035\n",
            "2019-11-08T03:22:55.423997: Epoch   1 Batch 2235/3125   train_loss = 1.150\n",
            "2019-11-08T03:22:56.466110: Epoch   1 Batch 2255/3125   train_loss = 1.108\n",
            "2019-11-08T03:22:57.514104: Epoch   1 Batch 2275/3125   train_loss = 0.917\n",
            "2019-11-08T03:22:58.516456: Epoch   1 Batch 2295/3125   train_loss = 1.221\n",
            "2019-11-08T03:22:59.543684: Epoch   1 Batch 2315/3125   train_loss = 1.091\n",
            "2019-11-08T03:23:00.613044: Epoch   1 Batch 2335/3125   train_loss = 1.065\n",
            "2019-11-08T03:23:01.668866: Epoch   1 Batch 2355/3125   train_loss = 1.072\n",
            "2019-11-08T03:23:02.718762: Epoch   1 Batch 2375/3125   train_loss = 1.213\n",
            "2019-11-08T03:23:03.765192: Epoch   1 Batch 2395/3125   train_loss = 1.026\n",
            "2019-11-08T03:23:04.794672: Epoch   1 Batch 2415/3125   train_loss = 1.085\n",
            "2019-11-08T03:23:05.872355: Epoch   1 Batch 2435/3125   train_loss = 1.021\n",
            "2019-11-08T03:23:06.921366: Epoch   1 Batch 2455/3125   train_loss = 1.084\n",
            "2019-11-08T03:23:07.958716: Epoch   1 Batch 2475/3125   train_loss = 1.014\n",
            "2019-11-08T03:23:09.001232: Epoch   1 Batch 2495/3125   train_loss = 1.046\n",
            "2019-11-08T03:23:10.041731: Epoch   1 Batch 2515/3125   train_loss = 1.094\n",
            "2019-11-08T03:23:11.107054: Epoch   1 Batch 2535/3125   train_loss = 1.066\n",
            "2019-11-08T03:23:12.153216: Epoch   1 Batch 2555/3125   train_loss = 0.894\n",
            "2019-11-08T03:23:13.246354: Epoch   1 Batch 2575/3125   train_loss = 0.958\n",
            "2019-11-08T03:23:14.268051: Epoch   1 Batch 2595/3125   train_loss = 1.011\n",
            "2019-11-08T03:23:15.339194: Epoch   1 Batch 2615/3125   train_loss = 1.167\n",
            "2019-11-08T03:23:16.417177: Epoch   1 Batch 2635/3125   train_loss = 0.941\n",
            "2019-11-08T03:23:17.426842: Epoch   1 Batch 2655/3125   train_loss = 0.946\n",
            "2019-11-08T03:23:18.423635: Epoch   1 Batch 2675/3125   train_loss = 0.997\n",
            "2019-11-08T03:23:19.443383: Epoch   1 Batch 2695/3125   train_loss = 1.004\n",
            "2019-11-08T03:23:20.480010: Epoch   1 Batch 2715/3125   train_loss = 0.965\n",
            "2019-11-08T03:23:21.484208: Epoch   1 Batch 2735/3125   train_loss = 0.927\n",
            "2019-11-08T03:23:22.500876: Epoch   1 Batch 2755/3125   train_loss = 1.067\n",
            "2019-11-08T03:23:23.522602: Epoch   1 Batch 2775/3125   train_loss = 1.100\n",
            "2019-11-08T03:23:24.580216: Epoch   1 Batch 2795/3125   train_loss = 1.071\n",
            "2019-11-08T03:23:25.616366: Epoch   1 Batch 2815/3125   train_loss = 0.943\n",
            "2019-11-08T03:23:26.651208: Epoch   1 Batch 2835/3125   train_loss = 1.058\n",
            "2019-11-08T03:23:27.688295: Epoch   1 Batch 2855/3125   train_loss = 1.035\n",
            "2019-11-08T03:23:28.740382: Epoch   1 Batch 2875/3125   train_loss = 1.073\n",
            "2019-11-08T03:23:29.801609: Epoch   1 Batch 2895/3125   train_loss = 1.070\n",
            "2019-11-08T03:23:30.850995: Epoch   1 Batch 2915/3125   train_loss = 1.006\n",
            "2019-11-08T03:23:31.913628: Epoch   1 Batch 2935/3125   train_loss = 1.087\n",
            "2019-11-08T03:23:32.967062: Epoch   1 Batch 2955/3125   train_loss = 1.112\n",
            "2019-11-08T03:23:34.012883: Epoch   1 Batch 2975/3125   train_loss = 1.030\n",
            "2019-11-08T03:23:35.047438: Epoch   1 Batch 2995/3125   train_loss = 0.961\n",
            "2019-11-08T03:23:36.090270: Epoch   1 Batch 3015/3125   train_loss = 0.977\n",
            "2019-11-08T03:23:37.111123: Epoch   1 Batch 3035/3125   train_loss = 1.079\n",
            "2019-11-08T03:23:38.124377: Epoch   1 Batch 3055/3125   train_loss = 1.074\n",
            "2019-11-08T03:23:39.161040: Epoch   1 Batch 3075/3125   train_loss = 0.971\n",
            "2019-11-08T03:23:40.200975: Epoch   1 Batch 3095/3125   train_loss = 1.034\n",
            "2019-11-08T03:23:41.208923: Epoch   1 Batch 3115/3125   train_loss = 0.868\n",
            "2019-11-08T03:23:41.907714: Epoch   1 Batch   19/781   test_loss = 1.069\n",
            "2019-11-08T03:23:42.163858: Epoch   1 Batch   39/781   test_loss = 0.830\n",
            "2019-11-08T03:23:42.410051: Epoch   1 Batch   59/781   test_loss = 0.905\n",
            "2019-11-08T03:23:42.639284: Epoch   1 Batch   79/781   test_loss = 1.013\n",
            "2019-11-08T03:23:42.880615: Epoch   1 Batch   99/781   test_loss = 1.016\n",
            "2019-11-08T03:23:43.139205: Epoch   1 Batch  119/781   test_loss = 0.957\n",
            "2019-11-08T03:23:43.376901: Epoch   1 Batch  139/781   test_loss = 1.094\n",
            "2019-11-08T03:23:43.584361: Epoch   1 Batch  159/781   test_loss = 1.011\n",
            "2019-11-08T03:23:43.803745: Epoch   1 Batch  179/781   test_loss = 0.941\n",
            "2019-11-08T03:23:44.030303: Epoch   1 Batch  199/781   test_loss = 0.940\n",
            "2019-11-08T03:23:44.284865: Epoch   1 Batch  219/781   test_loss = 1.032\n",
            "2019-11-08T03:23:44.532106: Epoch   1 Batch  239/781   test_loss = 1.276\n",
            "2019-11-08T03:23:44.771288: Epoch   1 Batch  259/781   test_loss = 0.999\n",
            "2019-11-08T03:23:45.004981: Epoch   1 Batch  279/781   test_loss = 1.168\n",
            "2019-11-08T03:23:45.276681: Epoch   1 Batch  299/781   test_loss = 1.189\n",
            "2019-11-08T03:23:45.490415: Epoch   1 Batch  319/781   test_loss = 0.965\n",
            "2019-11-08T03:23:45.711351: Epoch   1 Batch  339/781   test_loss = 0.923\n",
            "2019-11-08T03:23:45.946345: Epoch   1 Batch  359/781   test_loss = 0.928\n",
            "2019-11-08T03:23:46.181383: Epoch   1 Batch  379/781   test_loss = 1.027\n",
            "2019-11-08T03:23:46.418148: Epoch   1 Batch  399/781   test_loss = 0.913\n",
            "2019-11-08T03:23:46.647896: Epoch   1 Batch  419/781   test_loss = 1.001\n",
            "2019-11-08T03:23:46.862676: Epoch   1 Batch  439/781   test_loss = 1.018\n",
            "2019-11-08T03:23:47.059905: Epoch   1 Batch  459/781   test_loss = 1.088\n",
            "2019-11-08T03:23:47.310372: Epoch   1 Batch  479/781   test_loss = 1.037\n",
            "2019-11-08T03:23:47.542447: Epoch   1 Batch  499/781   test_loss = 0.932\n",
            "2019-11-08T03:23:47.754033: Epoch   1 Batch  519/781   test_loss = 1.085\n",
            "2019-11-08T03:23:48.016503: Epoch   1 Batch  539/781   test_loss = 0.905\n",
            "2019-11-08T03:23:48.227617: Epoch   1 Batch  559/781   test_loss = 1.098\n",
            "2019-11-08T03:23:48.459142: Epoch   1 Batch  579/781   test_loss = 0.993\n",
            "2019-11-08T03:23:48.679283: Epoch   1 Batch  599/781   test_loss = 0.989\n",
            "2019-11-08T03:23:48.921040: Epoch   1 Batch  619/781   test_loss = 1.141\n",
            "2019-11-08T03:23:49.172652: Epoch   1 Batch  639/781   test_loss = 0.927\n",
            "2019-11-08T03:23:49.421564: Epoch   1 Batch  659/781   test_loss = 1.139\n",
            "2019-11-08T03:23:49.667544: Epoch   1 Batch  679/781   test_loss = 1.155\n",
            "2019-11-08T03:23:49.920245: Epoch   1 Batch  699/781   test_loss = 0.840\n",
            "2019-11-08T03:23:50.163454: Epoch   1 Batch  719/781   test_loss = 0.943\n",
            "2019-11-08T03:23:50.388527: Epoch   1 Batch  739/781   test_loss = 1.005\n",
            "2019-11-08T03:23:50.635792: Epoch   1 Batch  759/781   test_loss = 0.903\n",
            "2019-11-08T03:23:50.903228: Epoch   1 Batch  779/781   test_loss = 0.840\n",
            "2019-11-08T03:23:52.148169: Epoch   2 Batch   10/3125   train_loss = 0.917\n",
            "2019-11-08T03:23:53.263679: Epoch   2 Batch   30/3125   train_loss = 1.057\n",
            "2019-11-08T03:23:54.325823: Epoch   2 Batch   50/3125   train_loss = 1.138\n",
            "2019-11-08T03:23:55.423287: Epoch   2 Batch   70/3125   train_loss = 1.036\n",
            "2019-11-08T03:23:56.570085: Epoch   2 Batch   90/3125   train_loss = 1.037\n",
            "2019-11-08T03:23:57.651454: Epoch   2 Batch  110/3125   train_loss = 0.926\n",
            "2019-11-08T03:23:58.736788: Epoch   2 Batch  130/3125   train_loss = 0.997\n",
            "2019-11-08T03:23:59.791766: Epoch   2 Batch  150/3125   train_loss = 1.103\n",
            "2019-11-08T03:24:00.859432: Epoch   2 Batch  170/3125   train_loss = 0.982\n",
            "2019-11-08T03:24:01.915201: Epoch   2 Batch  190/3125   train_loss = 1.043\n",
            "2019-11-08T03:24:02.982060: Epoch   2 Batch  210/3125   train_loss = 0.950\n",
            "2019-11-08T03:24:04.012935: Epoch   2 Batch  230/3125   train_loss = 1.047\n",
            "2019-11-08T03:24:05.029063: Epoch   2 Batch  250/3125   train_loss = 1.009\n",
            "2019-11-08T03:24:06.123031: Epoch   2 Batch  270/3125   train_loss = 0.828\n",
            "2019-11-08T03:24:07.199013: Epoch   2 Batch  290/3125   train_loss = 1.080\n",
            "2019-11-08T03:24:08.263018: Epoch   2 Batch  310/3125   train_loss = 0.983\n",
            "2019-11-08T03:24:09.334741: Epoch   2 Batch  330/3125   train_loss = 1.017\n",
            "2019-11-08T03:24:10.452828: Epoch   2 Batch  350/3125   train_loss = 0.901\n",
            "2019-11-08T03:24:11.543369: Epoch   2 Batch  370/3125   train_loss = 1.134\n",
            "2019-11-08T03:24:12.604657: Epoch   2 Batch  390/3125   train_loss = 1.169\n",
            "2019-11-08T03:24:13.638037: Epoch   2 Batch  410/3125   train_loss = 0.926\n",
            "2019-11-08T03:24:14.729581: Epoch   2 Batch  430/3125   train_loss = 1.176\n",
            "2019-11-08T03:24:15.819127: Epoch   2 Batch  450/3125   train_loss = 0.951\n",
            "2019-11-08T03:24:16.868901: Epoch   2 Batch  470/3125   train_loss = 0.930\n",
            "2019-11-08T03:24:17.914021: Epoch   2 Batch  490/3125   train_loss = 0.968\n",
            "2019-11-08T03:24:18.979301: Epoch   2 Batch  510/3125   train_loss = 1.092\n",
            "2019-11-08T03:24:20.042930: Epoch   2 Batch  530/3125   train_loss = 0.951\n",
            "2019-11-08T03:24:21.122078: Epoch   2 Batch  550/3125   train_loss = 0.980\n",
            "2019-11-08T03:24:22.174424: Epoch   2 Batch  570/3125   train_loss = 1.122\n",
            "2019-11-08T03:24:23.210685: Epoch   2 Batch  590/3125   train_loss = 1.049\n",
            "2019-11-08T03:24:24.257911: Epoch   2 Batch  610/3125   train_loss = 0.961\n",
            "2019-11-08T03:24:25.309289: Epoch   2 Batch  630/3125   train_loss = 1.052\n",
            "2019-11-08T03:24:26.378945: Epoch   2 Batch  650/3125   train_loss = 1.067\n",
            "2019-11-08T03:24:27.517428: Epoch   2 Batch  670/3125   train_loss = 1.021\n",
            "2019-11-08T03:24:28.598461: Epoch   2 Batch  690/3125   train_loss = 0.950\n",
            "2019-11-08T03:24:29.693232: Epoch   2 Batch  710/3125   train_loss = 0.948\n",
            "2019-11-08T03:24:30.707674: Epoch   2 Batch  730/3125   train_loss = 0.841\n",
            "2019-11-08T03:24:31.807885: Epoch   2 Batch  750/3125   train_loss = 0.969\n",
            "2019-11-08T03:24:32.918102: Epoch   2 Batch  770/3125   train_loss = 0.906\n",
            "2019-11-08T03:24:34.026477: Epoch   2 Batch  790/3125   train_loss = 0.899\n",
            "2019-11-08T03:24:35.042059: Epoch   2 Batch  810/3125   train_loss = 0.833\n",
            "2019-11-08T03:24:36.083694: Epoch   2 Batch  830/3125   train_loss = 0.785\n",
            "2019-11-08T03:24:37.085500: Epoch   2 Batch  850/3125   train_loss = 0.990\n",
            "2019-11-08T03:24:38.194196: Epoch   2 Batch  870/3125   train_loss = 0.859\n",
            "2019-11-08T03:24:39.282879: Epoch   2 Batch  890/3125   train_loss = 0.885\n",
            "2019-11-08T03:24:40.384894: Epoch   2 Batch  910/3125   train_loss = 0.993\n",
            "2019-11-08T03:24:41.470639: Epoch   2 Batch  930/3125   train_loss = 1.039\n",
            "2019-11-08T03:24:42.556646: Epoch   2 Batch  950/3125   train_loss = 0.983\n",
            "2019-11-08T03:24:43.582337: Epoch   2 Batch  970/3125   train_loss = 1.056\n",
            "2019-11-08T03:24:44.678777: Epoch   2 Batch  990/3125   train_loss = 0.872\n",
            "2019-11-08T03:24:45.758584: Epoch   2 Batch 1010/3125   train_loss = 1.108\n",
            "2019-11-08T03:24:46.808620: Epoch   2 Batch 1030/3125   train_loss = 0.938\n",
            "2019-11-08T03:24:47.875903: Epoch   2 Batch 1050/3125   train_loss = 0.922\n",
            "2019-11-08T03:24:48.972708: Epoch   2 Batch 1070/3125   train_loss = 1.025\n",
            "2019-11-08T03:24:50.061640: Epoch   2 Batch 1090/3125   train_loss = 1.124\n",
            "2019-11-08T03:24:51.144998: Epoch   2 Batch 1110/3125   train_loss = 1.131\n",
            "2019-11-08T03:24:52.261395: Epoch   2 Batch 1130/3125   train_loss = 0.971\n",
            "2019-11-08T03:24:53.331507: Epoch   2 Batch 1150/3125   train_loss = 0.990\n",
            "2019-11-08T03:24:54.431966: Epoch   2 Batch 1170/3125   train_loss = 0.997\n",
            "2019-11-08T03:24:55.581341: Epoch   2 Batch 1190/3125   train_loss = 1.031\n",
            "2019-11-08T03:24:56.687524: Epoch   2 Batch 1210/3125   train_loss = 0.877\n",
            "2019-11-08T03:24:57.807427: Epoch   2 Batch 1230/3125   train_loss = 0.912\n",
            "2019-11-08T03:24:58.909657: Epoch   2 Batch 1250/3125   train_loss = 1.034\n",
            "2019-11-08T03:24:59.996372: Epoch   2 Batch 1270/3125   train_loss = 0.934\n",
            "2019-11-08T03:25:01.083852: Epoch   2 Batch 1290/3125   train_loss = 0.977\n",
            "2019-11-08T03:25:02.151639: Epoch   2 Batch 1310/3125   train_loss = 1.013\n",
            "2019-11-08T03:25:03.252652: Epoch   2 Batch 1330/3125   train_loss = 1.031\n",
            "2019-11-08T03:25:04.319047: Epoch   2 Batch 1350/3125   train_loss = 0.917\n",
            "2019-11-08T03:25:05.396115: Epoch   2 Batch 1370/3125   train_loss = 0.915\n",
            "2019-11-08T03:25:06.494665: Epoch   2 Batch 1390/3125   train_loss = 0.997\n",
            "2019-11-08T03:25:07.598235: Epoch   2 Batch 1410/3125   train_loss = 0.985\n",
            "2019-11-08T03:25:08.692700: Epoch   2 Batch 1430/3125   train_loss = 1.007\n",
            "2019-11-08T03:25:09.746621: Epoch   2 Batch 1450/3125   train_loss = 1.031\n",
            "2019-11-08T03:25:10.785535: Epoch   2 Batch 1470/3125   train_loss = 0.938\n",
            "2019-11-08T03:25:11.869591: Epoch   2 Batch 1490/3125   train_loss = 1.066\n",
            "2019-11-08T03:25:13.019645: Epoch   2 Batch 1510/3125   train_loss = 0.980\n",
            "2019-11-08T03:25:14.118960: Epoch   2 Batch 1530/3125   train_loss = 1.093\n",
            "2019-11-08T03:25:15.241620: Epoch   2 Batch 1550/3125   train_loss = 0.873\n",
            "2019-11-08T03:25:16.304198: Epoch   2 Batch 1570/3125   train_loss = 0.958\n",
            "2019-11-08T03:25:17.420973: Epoch   2 Batch 1590/3125   train_loss = 0.974\n",
            "2019-11-08T03:25:18.462555: Epoch   2 Batch 1610/3125   train_loss = 1.047\n",
            "2019-11-08T03:25:19.488470: Epoch   2 Batch 1630/3125   train_loss = 1.028\n",
            "2019-11-08T03:25:20.585110: Epoch   2 Batch 1650/3125   train_loss = 0.849\n",
            "2019-11-08T03:25:21.665796: Epoch   2 Batch 1670/3125   train_loss = 0.846\n",
            "2019-11-08T03:25:22.736483: Epoch   2 Batch 1690/3125   train_loss = 0.991\n",
            "2019-11-08T03:25:23.754554: Epoch   2 Batch 1710/3125   train_loss = 0.903\n",
            "2019-11-08T03:25:24.811912: Epoch   2 Batch 1730/3125   train_loss = 0.952\n",
            "2019-11-08T03:25:25.900821: Epoch   2 Batch 1750/3125   train_loss = 0.828\n",
            "2019-11-08T03:25:26.976803: Epoch   2 Batch 1770/3125   train_loss = 1.089\n",
            "2019-11-08T03:25:28.016023: Epoch   2 Batch 1790/3125   train_loss = 1.029\n",
            "2019-11-08T03:25:29.093146: Epoch   2 Batch 1810/3125   train_loss = 1.009\n",
            "2019-11-08T03:25:30.153850: Epoch   2 Batch 1830/3125   train_loss = 0.986\n",
            "2019-11-08T03:25:31.243059: Epoch   2 Batch 1850/3125   train_loss = 0.912\n",
            "2019-11-08T03:25:32.301820: Epoch   2 Batch 1870/3125   train_loss = 0.988\n",
            "2019-11-08T03:25:33.310773: Epoch   2 Batch 1890/3125   train_loss = 0.821\n",
            "2019-11-08T03:25:34.373172: Epoch   2 Batch 1910/3125   train_loss = 0.894\n",
            "2019-11-08T03:25:35.413564: Epoch   2 Batch 1930/3125   train_loss = 1.028\n",
            "2019-11-08T03:25:36.443276: Epoch   2 Batch 1950/3125   train_loss = 0.890\n",
            "2019-11-08T03:25:37.503012: Epoch   2 Batch 1970/3125   train_loss = 1.001\n",
            "2019-11-08T03:25:38.530873: Epoch   2 Batch 1990/3125   train_loss = 0.882\n",
            "2019-11-08T03:25:39.615939: Epoch   2 Batch 2010/3125   train_loss = 0.849\n",
            "2019-11-08T03:25:40.699372: Epoch   2 Batch 2030/3125   train_loss = 0.961\n",
            "2019-11-08T03:25:41.794269: Epoch   2 Batch 2050/3125   train_loss = 1.001\n",
            "2019-11-08T03:25:42.918356: Epoch   2 Batch 2070/3125   train_loss = 0.887\n",
            "2019-11-08T03:25:43.987092: Epoch   2 Batch 2090/3125   train_loss = 0.871\n",
            "2019-11-08T03:25:45.073543: Epoch   2 Batch 2110/3125   train_loss = 1.094\n",
            "2019-11-08T03:25:46.096053: Epoch   2 Batch 2130/3125   train_loss = 0.964\n",
            "2019-11-08T03:25:47.180256: Epoch   2 Batch 2150/3125   train_loss = 0.966\n",
            "2019-11-08T03:25:48.268117: Epoch   2 Batch 2170/3125   train_loss = 0.873\n",
            "2019-11-08T03:25:49.348080: Epoch   2 Batch 2190/3125   train_loss = 0.949\n",
            "2019-11-08T03:25:50.437870: Epoch   2 Batch 2210/3125   train_loss = 0.970\n",
            "2019-11-08T03:25:51.512499: Epoch   2 Batch 2230/3125   train_loss = 0.937\n",
            "2019-11-08T03:25:52.589988: Epoch   2 Batch 2250/3125   train_loss = 1.058\n",
            "2019-11-08T03:25:53.686940: Epoch   2 Batch 2270/3125   train_loss = 0.940\n",
            "2019-11-08T03:25:54.746727: Epoch   2 Batch 2290/3125   train_loss = 0.872\n",
            "2019-11-08T03:25:55.776742: Epoch   2 Batch 2310/3125   train_loss = 0.868\n",
            "2019-11-08T03:25:56.855040: Epoch   2 Batch 2330/3125   train_loss = 1.050\n",
            "2019-11-08T03:25:57.959321: Epoch   2 Batch 2350/3125   train_loss = 1.052\n",
            "2019-11-08T03:25:59.075969: Epoch   2 Batch 2370/3125   train_loss = 0.925\n",
            "2019-11-08T03:26:00.178467: Epoch   2 Batch 2390/3125   train_loss = 1.032\n",
            "2019-11-08T03:26:01.252697: Epoch   2 Batch 2410/3125   train_loss = 1.042\n",
            "2019-11-08T03:26:02.283554: Epoch   2 Batch 2430/3125   train_loss = 0.928\n",
            "2019-11-08T03:26:03.395106: Epoch   2 Batch 2450/3125   train_loss = 0.903\n",
            "2019-11-08T03:26:04.460797: Epoch   2 Batch 2470/3125   train_loss = 0.968\n",
            "2019-11-08T03:26:05.594837: Epoch   2 Batch 2490/3125   train_loss = 1.066\n",
            "2019-11-08T03:26:06.621272: Epoch   2 Batch 2510/3125   train_loss = 1.007\n",
            "2019-11-08T03:26:07.698216: Epoch   2 Batch 2530/3125   train_loss = 0.818\n",
            "2019-11-08T03:26:08.749535: Epoch   2 Batch 2550/3125   train_loss = 1.011\n",
            "2019-11-08T03:26:09.771714: Epoch   2 Batch 2570/3125   train_loss = 0.978\n",
            "2019-11-08T03:26:10.824106: Epoch   2 Batch 2590/3125   train_loss = 0.964\n",
            "2019-11-08T03:26:11.938068: Epoch   2 Batch 2610/3125   train_loss = 1.029\n",
            "2019-11-08T03:26:13.009899: Epoch   2 Batch 2630/3125   train_loss = 0.713\n",
            "2019-11-08T03:26:14.102781: Epoch   2 Batch 2650/3125   train_loss = 0.959\n",
            "2019-11-08T03:26:15.194095: Epoch   2 Batch 2670/3125   train_loss = 1.014\n",
            "2019-11-08T03:26:16.354539: Epoch   2 Batch 2690/3125   train_loss = 1.013\n",
            "2019-11-08T03:26:17.436068: Epoch   2 Batch 2710/3125   train_loss = 0.802\n",
            "2019-11-08T03:26:18.491863: Epoch   2 Batch 2730/3125   train_loss = 1.169\n",
            "2019-11-08T03:26:19.524871: Epoch   2 Batch 2750/3125   train_loss = 1.027\n",
            "2019-11-08T03:26:20.612184: Epoch   2 Batch 2770/3125   train_loss = 0.951\n",
            "2019-11-08T03:26:21.667641: Epoch   2 Batch 2790/3125   train_loss = 0.909\n",
            "2019-11-08T03:26:22.662957: Epoch   2 Batch 2810/3125   train_loss = 0.997\n",
            "2019-11-08T03:26:23.677784: Epoch   2 Batch 2830/3125   train_loss = 0.828\n",
            "2019-11-08T03:26:24.758585: Epoch   2 Batch 2850/3125   train_loss = 1.012\n",
            "2019-11-08T03:26:25.822338: Epoch   2 Batch 2870/3125   train_loss = 0.832\n",
            "2019-11-08T03:26:26.912608: Epoch   2 Batch 2890/3125   train_loss = 0.778\n",
            "2019-11-08T03:26:28.025696: Epoch   2 Batch 2910/3125   train_loss = 0.979\n",
            "2019-11-08T03:26:29.138003: Epoch   2 Batch 2930/3125   train_loss = 0.793\n",
            "2019-11-08T03:26:30.202099: Epoch   2 Batch 2950/3125   train_loss = 1.104\n",
            "2019-11-08T03:26:31.278176: Epoch   2 Batch 2970/3125   train_loss = 0.943\n",
            "2019-11-08T03:26:32.325031: Epoch   2 Batch 2990/3125   train_loss = 0.957\n",
            "2019-11-08T03:26:33.418111: Epoch   2 Batch 3010/3125   train_loss = 0.857\n",
            "2019-11-08T03:26:34.500673: Epoch   2 Batch 3030/3125   train_loss = 0.963\n",
            "2019-11-08T03:26:35.562492: Epoch   2 Batch 3050/3125   train_loss = 1.015\n",
            "2019-11-08T03:26:36.584968: Epoch   2 Batch 3070/3125   train_loss = 0.875\n",
            "2019-11-08T03:26:37.628984: Epoch   2 Batch 3090/3125   train_loss = 0.810\n",
            "2019-11-08T03:26:38.676959: Epoch   2 Batch 3110/3125   train_loss = 0.788\n",
            "2019-11-08T03:26:39.665895: Epoch   2 Batch   18/781   test_loss = 0.766\n",
            "2019-11-08T03:26:39.894473: Epoch   2 Batch   38/781   test_loss = 0.867\n",
            "2019-11-08T03:26:40.137576: Epoch   2 Batch   58/781   test_loss = 0.864\n",
            "2019-11-08T03:26:40.380482: Epoch   2 Batch   78/781   test_loss = 0.904\n",
            "2019-11-08T03:26:40.654594: Epoch   2 Batch   98/781   test_loss = 0.898\n",
            "2019-11-08T03:26:40.902368: Epoch   2 Batch  118/781   test_loss = 0.808\n",
            "2019-11-08T03:26:41.120316: Epoch   2 Batch  138/781   test_loss = 0.921\n",
            "2019-11-08T03:26:41.345793: Epoch   2 Batch  158/781   test_loss = 0.865\n",
            "2019-11-08T03:26:41.583621: Epoch   2 Batch  178/781   test_loss = 0.841\n",
            "2019-11-08T03:26:41.822372: Epoch   2 Batch  198/781   test_loss = 0.900\n",
            "2019-11-08T03:26:42.070807: Epoch   2 Batch  218/781   test_loss = 1.003\n",
            "2019-11-08T03:26:42.326273: Epoch   2 Batch  238/781   test_loss = 0.963\n",
            "2019-11-08T03:26:42.565561: Epoch   2 Batch  258/781   test_loss = 1.006\n",
            "2019-11-08T03:26:42.821789: Epoch   2 Batch  278/781   test_loss = 1.049\n",
            "2019-11-08T03:26:43.092470: Epoch   2 Batch  298/781   test_loss = 0.896\n",
            "2019-11-08T03:26:43.309907: Epoch   2 Batch  318/781   test_loss = 0.921\n",
            "2019-11-08T03:26:43.540338: Epoch   2 Batch  338/781   test_loss = 0.932\n",
            "2019-11-08T03:26:43.768725: Epoch   2 Batch  358/781   test_loss = 0.948\n",
            "2019-11-08T03:26:43.968675: Epoch   2 Batch  378/781   test_loss = 0.907\n",
            "2019-11-08T03:26:44.195841: Epoch   2 Batch  398/781   test_loss = 0.836\n",
            "2019-11-08T03:26:44.445056: Epoch   2 Batch  418/781   test_loss = 0.942\n",
            "2019-11-08T03:26:44.661711: Epoch   2 Batch  438/781   test_loss = 1.018\n",
            "2019-11-08T03:26:44.899265: Epoch   2 Batch  458/781   test_loss = 0.894\n",
            "2019-11-08T03:26:45.143742: Epoch   2 Batch  478/781   test_loss = 0.938\n",
            "2019-11-08T03:26:45.400935: Epoch   2 Batch  498/781   test_loss = 0.776\n",
            "2019-11-08T03:26:45.629831: Epoch   2 Batch  518/781   test_loss = 0.870\n",
            "2019-11-08T03:26:45.849950: Epoch   2 Batch  538/781   test_loss = 0.779\n",
            "2019-11-08T03:26:46.064636: Epoch   2 Batch  558/781   test_loss = 0.862\n",
            "2019-11-08T03:26:46.281376: Epoch   2 Batch  578/781   test_loss = 0.921\n",
            "2019-11-08T03:26:46.496237: Epoch   2 Batch  598/781   test_loss = 1.074\n",
            "2019-11-08T03:26:46.723053: Epoch   2 Batch  618/781   test_loss = 0.828\n",
            "2019-11-08T03:26:46.929825: Epoch   2 Batch  638/781   test_loss = 0.892\n",
            "2019-11-08T03:26:47.138734: Epoch   2 Batch  658/781   test_loss = 1.026\n",
            "2019-11-08T03:26:47.390385: Epoch   2 Batch  678/781   test_loss = 0.918\n",
            "2019-11-08T03:26:47.639654: Epoch   2 Batch  698/781   test_loss = 0.882\n",
            "2019-11-08T03:26:47.896418: Epoch   2 Batch  718/781   test_loss = 1.030\n",
            "2019-11-08T03:26:48.164319: Epoch   2 Batch  738/781   test_loss = 0.857\n",
            "2019-11-08T03:26:48.400441: Epoch   2 Batch  758/781   test_loss = 0.944\n",
            "2019-11-08T03:26:48.621897: Epoch   2 Batch  778/781   test_loss = 0.947\n",
            "2019-11-08T03:26:49.586825: Epoch   3 Batch    5/3125   train_loss = 0.907\n",
            "2019-11-08T03:26:50.629261: Epoch   3 Batch   25/3125   train_loss = 0.939\n",
            "2019-11-08T03:26:51.677535: Epoch   3 Batch   45/3125   train_loss = 0.850\n",
            "2019-11-08T03:26:52.752712: Epoch   3 Batch   65/3125   train_loss = 0.942\n",
            "2019-11-08T03:26:53.820658: Epoch   3 Batch   85/3125   train_loss = 0.797\n",
            "2019-11-08T03:26:54.896975: Epoch   3 Batch  105/3125   train_loss = 0.751\n",
            "2019-11-08T03:26:55.920769: Epoch   3 Batch  125/3125   train_loss = 0.866\n",
            "2019-11-08T03:26:56.998934: Epoch   3 Batch  145/3125   train_loss = 0.952\n",
            "2019-11-08T03:26:58.044919: Epoch   3 Batch  165/3125   train_loss = 0.912\n",
            "2019-11-08T03:26:59.126855: Epoch   3 Batch  185/3125   train_loss = 0.827\n",
            "2019-11-08T03:27:00.174642: Epoch   3 Batch  205/3125   train_loss = 0.826\n",
            "2019-11-08T03:27:01.210869: Epoch   3 Batch  225/3125   train_loss = 0.787\n",
            "2019-11-08T03:27:02.246096: Epoch   3 Batch  245/3125   train_loss = 1.103\n",
            "2019-11-08T03:27:03.331044: Epoch   3 Batch  265/3125   train_loss = 0.921\n",
            "2019-11-08T03:27:04.387090: Epoch   3 Batch  285/3125   train_loss = 0.929\n",
            "2019-11-08T03:27:05.492757: Epoch   3 Batch  305/3125   train_loss = 0.857\n",
            "2019-11-08T03:27:06.580037: Epoch   3 Batch  325/3125   train_loss = 0.942\n",
            "2019-11-08T03:27:07.587910: Epoch   3 Batch  345/3125   train_loss = 0.975\n",
            "2019-11-08T03:27:08.646650: Epoch   3 Batch  365/3125   train_loss = 0.895\n",
            "2019-11-08T03:27:09.721279: Epoch   3 Batch  385/3125   train_loss = 0.905\n",
            "2019-11-08T03:27:10.787525: Epoch   3 Batch  405/3125   train_loss = 0.905\n",
            "2019-11-08T03:27:11.882601: Epoch   3 Batch  425/3125   train_loss = 0.973\n",
            "2019-11-08T03:27:13.014817: Epoch   3 Batch  445/3125   train_loss = 0.926\n",
            "2019-11-08T03:27:14.036350: Epoch   3 Batch  465/3125   train_loss = 0.859\n",
            "2019-11-08T03:27:15.064752: Epoch   3 Batch  485/3125   train_loss = 0.972\n",
            "2019-11-08T03:27:16.090088: Epoch   3 Batch  505/3125   train_loss = 0.868\n",
            "2019-11-08T03:27:17.088491: Epoch   3 Batch  525/3125   train_loss = 0.949\n",
            "2019-11-08T03:27:18.115281: Epoch   3 Batch  545/3125   train_loss = 0.925\n",
            "2019-11-08T03:27:19.093372: Epoch   3 Batch  565/3125   train_loss = 1.078\n",
            "2019-11-08T03:27:20.165404: Epoch   3 Batch  585/3125   train_loss = 0.844\n",
            "2019-11-08T03:27:21.228210: Epoch   3 Batch  605/3125   train_loss = 0.933\n",
            "2019-11-08T03:27:22.251860: Epoch   3 Batch  625/3125   train_loss = 0.934\n",
            "2019-11-08T03:27:23.256607: Epoch   3 Batch  645/3125   train_loss = 0.943\n",
            "2019-11-08T03:27:24.289298: Epoch   3 Batch  665/3125   train_loss = 0.966\n",
            "2019-11-08T03:27:25.291237: Epoch   3 Batch  685/3125   train_loss = 0.901\n",
            "2019-11-08T03:27:26.338039: Epoch   3 Batch  705/3125   train_loss = 1.104\n",
            "2019-11-08T03:27:27.379167: Epoch   3 Batch  725/3125   train_loss = 0.806\n",
            "2019-11-08T03:27:28.422855: Epoch   3 Batch  745/3125   train_loss = 0.958\n",
            "2019-11-08T03:27:29.458489: Epoch   3 Batch  765/3125   train_loss = 0.874\n",
            "2019-11-08T03:27:30.441795: Epoch   3 Batch  785/3125   train_loss = 1.091\n",
            "2019-11-08T03:27:31.557514: Epoch   3 Batch  805/3125   train_loss = 0.849\n",
            "2019-11-08T03:27:32.565913: Epoch   3 Batch  825/3125   train_loss = 0.925\n",
            "2019-11-08T03:27:33.634272: Epoch   3 Batch  845/3125   train_loss = 0.947\n",
            "2019-11-08T03:27:34.684399: Epoch   3 Batch  865/3125   train_loss = 0.979\n",
            "2019-11-08T03:27:35.745941: Epoch   3 Batch  885/3125   train_loss = 0.985\n",
            "2019-11-08T03:27:36.736235: Epoch   3 Batch  905/3125   train_loss = 1.020\n",
            "2019-11-08T03:27:37.736856: Epoch   3 Batch  925/3125   train_loss = 0.932\n",
            "2019-11-08T03:27:38.784891: Epoch   3 Batch  945/3125   train_loss = 0.967\n",
            "2019-11-08T03:27:39.849629: Epoch   3 Batch  965/3125   train_loss = 0.783\n",
            "2019-11-08T03:27:40.848163: Epoch   3 Batch  985/3125   train_loss = 1.021\n",
            "2019-11-08T03:27:41.863006: Epoch   3 Batch 1005/3125   train_loss = 0.768\n",
            "2019-11-08T03:27:42.946177: Epoch   3 Batch 1025/3125   train_loss = 0.881\n",
            "2019-11-08T03:27:43.965756: Epoch   3 Batch 1045/3125   train_loss = 1.133\n",
            "2019-11-08T03:27:44.977799: Epoch   3 Batch 1065/3125   train_loss = 0.863\n",
            "2019-11-08T03:27:45.976547: Epoch   3 Batch 1085/3125   train_loss = 0.842\n",
            "2019-11-08T03:27:47.002674: Epoch   3 Batch 1105/3125   train_loss = 0.876\n",
            "2019-11-08T03:27:47.953969: Epoch   3 Batch 1125/3125   train_loss = 0.836\n",
            "2019-11-08T03:27:48.959667: Epoch   3 Batch 1145/3125   train_loss = 0.902\n",
            "2019-11-08T03:27:49.975883: Epoch   3 Batch 1165/3125   train_loss = 1.039\n",
            "2019-11-08T03:27:50.987526: Epoch   3 Batch 1185/3125   train_loss = 0.814\n",
            "2019-11-08T03:27:52.017101: Epoch   3 Batch 1205/3125   train_loss = 0.918\n",
            "2019-11-08T03:27:53.033240: Epoch   3 Batch 1225/3125   train_loss = 0.950\n",
            "2019-11-08T03:27:54.028750: Epoch   3 Batch 1245/3125   train_loss = 1.027\n",
            "2019-11-08T03:27:55.065549: Epoch   3 Batch 1265/3125   train_loss = 0.878\n",
            "2019-11-08T03:27:56.098302: Epoch   3 Batch 1285/3125   train_loss = 1.059\n",
            "2019-11-08T03:27:57.143477: Epoch   3 Batch 1305/3125   train_loss = 0.787\n",
            "2019-11-08T03:27:58.205704: Epoch   3 Batch 1325/3125   train_loss = 0.907\n",
            "2019-11-08T03:27:59.257598: Epoch   3 Batch 1345/3125   train_loss = 0.975\n",
            "2019-11-08T03:28:00.292673: Epoch   3 Batch 1365/3125   train_loss = 0.811\n",
            "2019-11-08T03:28:01.367152: Epoch   3 Batch 1385/3125   train_loss = 0.821\n",
            "2019-11-08T03:28:02.397988: Epoch   3 Batch 1405/3125   train_loss = 0.893\n",
            "2019-11-08T03:28:03.439164: Epoch   3 Batch 1425/3125   train_loss = 1.031\n",
            "2019-11-08T03:28:04.511222: Epoch   3 Batch 1445/3125   train_loss = 1.007\n",
            "2019-11-08T03:28:05.556233: Epoch   3 Batch 1465/3125   train_loss = 0.891\n",
            "2019-11-08T03:28:06.592173: Epoch   3 Batch 1485/3125   train_loss = 0.986\n",
            "2019-11-08T03:28:07.615627: Epoch   3 Batch 1505/3125   train_loss = 0.773\n",
            "2019-11-08T03:28:08.619030: Epoch   3 Batch 1525/3125   train_loss = 0.821\n",
            "2019-11-08T03:28:09.662634: Epoch   3 Batch 1545/3125   train_loss = 0.917\n",
            "2019-11-08T03:28:10.677604: Epoch   3 Batch 1565/3125   train_loss = 0.928\n",
            "2019-11-08T03:28:11.694479: Epoch   3 Batch 1585/3125   train_loss = 0.884\n",
            "2019-11-08T03:28:12.764600: Epoch   3 Batch 1605/3125   train_loss = 0.913\n",
            "2019-11-08T03:28:13.825934: Epoch   3 Batch 1625/3125   train_loss = 0.983\n",
            "2019-11-08T03:28:14.883352: Epoch   3 Batch 1645/3125   train_loss = 0.979\n",
            "2019-11-08T03:28:15.924306: Epoch   3 Batch 1665/3125   train_loss = 0.926\n",
            "2019-11-08T03:28:16.977755: Epoch   3 Batch 1685/3125   train_loss = 1.012\n",
            "2019-11-08T03:28:18.055182: Epoch   3 Batch 1705/3125   train_loss = 0.930\n",
            "2019-11-08T03:28:19.052925: Epoch   3 Batch 1725/3125   train_loss = 0.835\n",
            "2019-11-08T03:28:20.062678: Epoch   3 Batch 1745/3125   train_loss = 0.841\n",
            "2019-11-08T03:28:21.071714: Epoch   3 Batch 1765/3125   train_loss = 0.908\n",
            "2019-11-08T03:28:22.104459: Epoch   3 Batch 1785/3125   train_loss = 1.039\n",
            "2019-11-08T03:28:23.085623: Epoch   3 Batch 1805/3125   train_loss = 0.954\n",
            "2019-11-08T03:28:24.152825: Epoch   3 Batch 1825/3125   train_loss = 1.024\n",
            "2019-11-08T03:28:25.193733: Epoch   3 Batch 1845/3125   train_loss = 0.900\n",
            "2019-11-08T03:28:26.226000: Epoch   3 Batch 1865/3125   train_loss = 0.748\n",
            "2019-11-08T03:28:27.277455: Epoch   3 Batch 1885/3125   train_loss = 0.952\n",
            "2019-11-08T03:28:28.342042: Epoch   3 Batch 1905/3125   train_loss = 0.829\n",
            "2019-11-08T03:28:29.347440: Epoch   3 Batch 1925/3125   train_loss = 0.803\n",
            "2019-11-08T03:28:30.397934: Epoch   3 Batch 1945/3125   train_loss = 0.875\n",
            "2019-11-08T03:28:31.434624: Epoch   3 Batch 1965/3125   train_loss = 0.814\n",
            "2019-11-08T03:28:32.440996: Epoch   3 Batch 1985/3125   train_loss = 0.840\n",
            "2019-11-08T03:28:33.485190: Epoch   3 Batch 2005/3125   train_loss = 0.923\n",
            "2019-11-08T03:28:34.519005: Epoch   3 Batch 2025/3125   train_loss = 0.984\n",
            "2019-11-08T03:28:35.532610: Epoch   3 Batch 2045/3125   train_loss = 0.795\n",
            "2019-11-08T03:28:36.492267: Epoch   3 Batch 2065/3125   train_loss = 0.727\n",
            "2019-11-08T03:28:37.453538: Epoch   3 Batch 2085/3125   train_loss = 0.973\n",
            "2019-11-08T03:28:38.429998: Epoch   3 Batch 2105/3125   train_loss = 0.893\n",
            "2019-11-08T03:28:39.480166: Epoch   3 Batch 2125/3125   train_loss = 0.973\n",
            "2019-11-08T03:28:40.520382: Epoch   3 Batch 2145/3125   train_loss = 1.020\n",
            "2019-11-08T03:28:41.528975: Epoch   3 Batch 2165/3125   train_loss = 0.816\n",
            "2019-11-08T03:28:42.570771: Epoch   3 Batch 2185/3125   train_loss = 0.938\n",
            "2019-11-08T03:28:43.593665: Epoch   3 Batch 2205/3125   train_loss = 0.967\n",
            "2019-11-08T03:28:44.630389: Epoch   3 Batch 2225/3125   train_loss = 0.866\n",
            "2019-11-08T03:28:45.642678: Epoch   3 Batch 2245/3125   train_loss = 0.826\n",
            "2019-11-08T03:28:46.634086: Epoch   3 Batch 2265/3125   train_loss = 0.897\n",
            "2019-11-08T03:28:47.622390: Epoch   3 Batch 2285/3125   train_loss = 1.076\n",
            "2019-11-08T03:28:48.640056: Epoch   3 Batch 2305/3125   train_loss = 0.857\n",
            "2019-11-08T03:28:49.684293: Epoch   3 Batch 2325/3125   train_loss = 0.839\n",
            "2019-11-08T03:28:50.723243: Epoch   3 Batch 2345/3125   train_loss = 0.902\n",
            "2019-11-08T03:28:51.775865: Epoch   3 Batch 2365/3125   train_loss = 0.785\n",
            "2019-11-08T03:28:52.828498: Epoch   3 Batch 2385/3125   train_loss = 0.932\n",
            "2019-11-08T03:28:53.815457: Epoch   3 Batch 2405/3125   train_loss = 0.909\n",
            "2019-11-08T03:28:54.866240: Epoch   3 Batch 2425/3125   train_loss = 0.872\n",
            "2019-11-08T03:28:55.892797: Epoch   3 Batch 2445/3125   train_loss = 0.937\n",
            "2019-11-08T03:28:56.952187: Epoch   3 Batch 2465/3125   train_loss = 0.799\n",
            "2019-11-08T03:28:58.026543: Epoch   3 Batch 2485/3125   train_loss = 0.871\n",
            "2019-11-08T03:28:59.077606: Epoch   3 Batch 2505/3125   train_loss = 0.855\n",
            "2019-11-08T03:29:00.141532: Epoch   3 Batch 2525/3125   train_loss = 0.831\n",
            "2019-11-08T03:29:01.224907: Epoch   3 Batch 2545/3125   train_loss = 0.988\n",
            "2019-11-08T03:29:02.241640: Epoch   3 Batch 2565/3125   train_loss = 0.839\n",
            "2019-11-08T03:29:03.268224: Epoch   3 Batch 2585/3125   train_loss = 0.781\n",
            "2019-11-08T03:29:04.316365: Epoch   3 Batch 2605/3125   train_loss = 0.824\n",
            "2019-11-08T03:29:05.340668: Epoch   3 Batch 2625/3125   train_loss = 1.010\n",
            "2019-11-08T03:29:06.365398: Epoch   3 Batch 2645/3125   train_loss = 0.924\n",
            "2019-11-08T03:29:07.408178: Epoch   3 Batch 2665/3125   train_loss = 0.975\n",
            "2019-11-08T03:29:08.411073: Epoch   3 Batch 2685/3125   train_loss = 0.943\n",
            "2019-11-08T03:29:09.467156: Epoch   3 Batch 2705/3125   train_loss = 0.811\n",
            "2019-11-08T03:29:10.510730: Epoch   3 Batch 2725/3125   train_loss = 0.952\n",
            "2019-11-08T03:29:11.557267: Epoch   3 Batch 2745/3125   train_loss = 0.884\n",
            "2019-11-08T03:29:12.573090: Epoch   3 Batch 2765/3125   train_loss = 0.806\n",
            "2019-11-08T03:29:13.586751: Epoch   3 Batch 2785/3125   train_loss = 0.959\n",
            "2019-11-08T03:29:14.602484: Epoch   3 Batch 2805/3125   train_loss = 0.834\n",
            "2019-11-08T03:29:15.622652: Epoch   3 Batch 2825/3125   train_loss = 0.879\n",
            "2019-11-08T03:29:16.661763: Epoch   3 Batch 2845/3125   train_loss = 0.858\n",
            "2019-11-08T03:29:17.708208: Epoch   3 Batch 2865/3125   train_loss = 0.848\n",
            "2019-11-08T03:29:18.721420: Epoch   3 Batch 2885/3125   train_loss = 0.953\n",
            "2019-11-08T03:29:19.649816: Epoch   3 Batch 2905/3125   train_loss = 0.953\n",
            "2019-11-08T03:29:20.651621: Epoch   3 Batch 2925/3125   train_loss = 0.874\n",
            "2019-11-08T03:29:21.621673: Epoch   3 Batch 2945/3125   train_loss = 0.924\n",
            "2019-11-08T03:29:22.569267: Epoch   3 Batch 2965/3125   train_loss = 0.957\n",
            "2019-11-08T03:29:23.518451: Epoch   3 Batch 2985/3125   train_loss = 0.811\n",
            "2019-11-08T03:29:24.546717: Epoch   3 Batch 3005/3125   train_loss = 0.775\n",
            "2019-11-08T03:29:25.542024: Epoch   3 Batch 3025/3125   train_loss = 0.965\n",
            "2019-11-08T03:29:26.549623: Epoch   3 Batch 3045/3125   train_loss = 0.903\n",
            "2019-11-08T03:29:27.565165: Epoch   3 Batch 3065/3125   train_loss = 0.791\n",
            "2019-11-08T03:29:28.553445: Epoch   3 Batch 3085/3125   train_loss = 0.889\n",
            "2019-11-08T03:29:29.570858: Epoch   3 Batch 3105/3125   train_loss = 0.945\n",
            "2019-11-08T03:29:30.697428: Epoch   3 Batch   17/781   test_loss = 0.901\n",
            "2019-11-08T03:29:30.932425: Epoch   3 Batch   37/781   test_loss = 0.896\n",
            "2019-11-08T03:29:31.144414: Epoch   3 Batch   57/781   test_loss = 0.916\n",
            "2019-11-08T03:29:31.393630: Epoch   3 Batch   77/781   test_loss = 0.872\n",
            "2019-11-08T03:29:31.630297: Epoch   3 Batch   97/781   test_loss = 0.772\n",
            "2019-11-08T03:29:31.875856: Epoch   3 Batch  117/781   test_loss = 0.985\n",
            "2019-11-08T03:29:32.125558: Epoch   3 Batch  137/781   test_loss = 0.920\n",
            "2019-11-08T03:29:32.370735: Epoch   3 Batch  157/781   test_loss = 0.915\n",
            "2019-11-08T03:29:32.609507: Epoch   3 Batch  177/781   test_loss = 0.872\n",
            "2019-11-08T03:29:32.824877: Epoch   3 Batch  197/781   test_loss = 0.921\n",
            "2019-11-08T03:29:33.082280: Epoch   3 Batch  217/781   test_loss = 0.722\n",
            "2019-11-08T03:29:33.357740: Epoch   3 Batch  237/781   test_loss = 0.766\n",
            "2019-11-08T03:29:33.626804: Epoch   3 Batch  257/781   test_loss = 1.066\n",
            "2019-11-08T03:29:33.898905: Epoch   3 Batch  277/781   test_loss = 0.964\n",
            "2019-11-08T03:29:34.185904: Epoch   3 Batch  297/781   test_loss = 0.978\n",
            "2019-11-08T03:29:34.445871: Epoch   3 Batch  317/781   test_loss = 1.057\n",
            "2019-11-08T03:29:34.680913: Epoch   3 Batch  337/781   test_loss = 0.897\n",
            "2019-11-08T03:29:34.876753: Epoch   3 Batch  357/781   test_loss = 0.868\n",
            "2019-11-08T03:29:35.121397: Epoch   3 Batch  377/781   test_loss = 0.967\n",
            "2019-11-08T03:29:35.366093: Epoch   3 Batch  397/781   test_loss = 0.883\n",
            "2019-11-08T03:29:35.589807: Epoch   3 Batch  417/781   test_loss = 0.839\n",
            "2019-11-08T03:29:35.808326: Epoch   3 Batch  437/781   test_loss = 0.799\n",
            "2019-11-08T03:29:36.048183: Epoch   3 Batch  457/781   test_loss = 0.722\n",
            "2019-11-08T03:29:36.282718: Epoch   3 Batch  477/781   test_loss = 0.887\n",
            "2019-11-08T03:29:36.506469: Epoch   3 Batch  497/781   test_loss = 0.815\n",
            "2019-11-08T03:29:36.718739: Epoch   3 Batch  517/781   test_loss = 0.797\n",
            "2019-11-08T03:29:36.948256: Epoch   3 Batch  537/781   test_loss = 0.866\n",
            "2019-11-08T03:29:37.171735: Epoch   3 Batch  557/781   test_loss = 0.994\n",
            "2019-11-08T03:29:37.427074: Epoch   3 Batch  577/781   test_loss = 0.901\n",
            "2019-11-08T03:29:37.682193: Epoch   3 Batch  597/781   test_loss = 0.878\n",
            "2019-11-08T03:29:37.947106: Epoch   3 Batch  617/781   test_loss = 0.880\n",
            "2019-11-08T03:29:38.182872: Epoch   3 Batch  637/781   test_loss = 0.764\n",
            "2019-11-08T03:29:38.402107: Epoch   3 Batch  657/781   test_loss = 0.973\n",
            "2019-11-08T03:29:38.625069: Epoch   3 Batch  677/781   test_loss = 0.943\n",
            "2019-11-08T03:29:38.862910: Epoch   3 Batch  697/781   test_loss = 0.944\n",
            "2019-11-08T03:29:39.094696: Epoch   3 Batch  717/781   test_loss = 0.894\n",
            "2019-11-08T03:29:39.303305: Epoch   3 Batch  737/781   test_loss = 0.736\n",
            "2019-11-08T03:29:39.550361: Epoch   3 Batch  757/781   test_loss = 1.060\n",
            "2019-11-08T03:29:39.778702: Epoch   3 Batch  777/781   test_loss = 0.890\n",
            "2019-11-08T03:29:40.450935: Epoch   4 Batch    0/3125   train_loss = 0.971\n",
            "2019-11-08T03:29:41.472773: Epoch   4 Batch   20/3125   train_loss = 0.834\n",
            "2019-11-08T03:29:42.562504: Epoch   4 Batch   40/3125   train_loss = 0.868\n",
            "2019-11-08T03:29:43.680148: Epoch   4 Batch   60/3125   train_loss = 0.733\n",
            "2019-11-08T03:29:44.759683: Epoch   4 Batch   80/3125   train_loss = 0.829\n",
            "2019-11-08T03:29:45.844694: Epoch   4 Batch  100/3125   train_loss = 0.915\n",
            "2019-11-08T03:29:46.896951: Epoch   4 Batch  120/3125   train_loss = 0.970\n",
            "2019-11-08T03:29:47.918249: Epoch   4 Batch  140/3125   train_loss = 0.956\n",
            "2019-11-08T03:29:48.969485: Epoch   4 Batch  160/3125   train_loss = 0.753\n",
            "2019-11-08T03:29:50.070360: Epoch   4 Batch  180/3125   train_loss = 0.862\n",
            "2019-11-08T03:29:51.179122: Epoch   4 Batch  200/3125   train_loss = 1.123\n",
            "2019-11-08T03:29:52.254704: Epoch   4 Batch  220/3125   train_loss = 0.884\n",
            "2019-11-08T03:29:53.308944: Epoch   4 Batch  240/3125   train_loss = 1.015\n",
            "2019-11-08T03:29:54.380172: Epoch   4 Batch  260/3125   train_loss = 0.942\n",
            "2019-11-08T03:29:55.442436: Epoch   4 Batch  280/3125   train_loss = 0.978\n",
            "2019-11-08T03:29:56.497403: Epoch   4 Batch  300/3125   train_loss = 1.088\n",
            "2019-11-08T03:29:57.545238: Epoch   4 Batch  320/3125   train_loss = 0.956\n",
            "2019-11-08T03:29:58.610758: Epoch   4 Batch  340/3125   train_loss = 0.717\n",
            "2019-11-08T03:29:59.679847: Epoch   4 Batch  360/3125   train_loss = 0.859\n",
            "2019-11-08T03:30:00.723950: Epoch   4 Batch  380/3125   train_loss = 0.862\n",
            "2019-11-08T03:30:01.777297: Epoch   4 Batch  400/3125   train_loss = 0.830\n",
            "2019-11-08T03:30:02.822195: Epoch   4 Batch  420/3125   train_loss = 0.845\n",
            "2019-11-08T03:30:03.854520: Epoch   4 Batch  440/3125   train_loss = 0.869\n",
            "2019-11-08T03:30:04.934890: Epoch   4 Batch  460/3125   train_loss = 0.925\n",
            "2019-11-08T03:30:05.991005: Epoch   4 Batch  480/3125   train_loss = 0.957\n",
            "2019-11-08T03:30:07.085619: Epoch   4 Batch  500/3125   train_loss = 0.662\n",
            "2019-11-08T03:30:08.162593: Epoch   4 Batch  520/3125   train_loss = 0.932\n",
            "2019-11-08T03:30:09.195176: Epoch   4 Batch  540/3125   train_loss = 0.807\n",
            "2019-11-08T03:30:10.326233: Epoch   4 Batch  560/3125   train_loss = 1.062\n",
            "2019-11-08T03:30:11.444979: Epoch   4 Batch  580/3125   train_loss = 1.002\n",
            "2019-11-08T03:30:12.553376: Epoch   4 Batch  600/3125   train_loss = 0.884\n",
            "2019-11-08T03:30:13.660087: Epoch   4 Batch  620/3125   train_loss = 0.891\n",
            "2019-11-08T03:30:14.747959: Epoch   4 Batch  640/3125   train_loss = 0.803\n",
            "2019-11-08T03:30:15.812049: Epoch   4 Batch  660/3125   train_loss = 0.896\n",
            "2019-11-08T03:30:16.835551: Epoch   4 Batch  680/3125   train_loss = 0.935\n",
            "2019-11-08T03:30:17.870971: Epoch   4 Batch  700/3125   train_loss = 0.922\n",
            "2019-11-08T03:30:18.990514: Epoch   4 Batch  720/3125   train_loss = 0.783\n",
            "2019-11-08T03:30:20.077784: Epoch   4 Batch  740/3125   train_loss = 0.909\n",
            "2019-11-08T03:30:21.181501: Epoch   4 Batch  760/3125   train_loss = 0.742\n",
            "2019-11-08T03:30:22.194524: Epoch   4 Batch  780/3125   train_loss = 0.904\n",
            "2019-11-08T03:30:23.153263: Epoch   4 Batch  800/3125   train_loss = 0.800\n",
            "2019-11-08T03:30:24.153893: Epoch   4 Batch  820/3125   train_loss = 0.844\n",
            "2019-11-08T03:30:25.182292: Epoch   4 Batch  840/3125   train_loss = 0.835\n",
            "2019-11-08T03:30:26.172062: Epoch   4 Batch  860/3125   train_loss = 0.873\n",
            "2019-11-08T03:30:27.180314: Epoch   4 Batch  880/3125   train_loss = 0.816\n",
            "2019-11-08T03:30:28.195378: Epoch   4 Batch  900/3125   train_loss = 0.920\n",
            "2019-11-08T03:30:29.215692: Epoch   4 Batch  920/3125   train_loss = 0.976\n",
            "2019-11-08T03:30:30.235215: Epoch   4 Batch  940/3125   train_loss = 0.861\n",
            "2019-11-08T03:30:31.265258: Epoch   4 Batch  960/3125   train_loss = 0.908\n",
            "2019-11-08T03:30:32.311734: Epoch   4 Batch  980/3125   train_loss = 1.008\n",
            "2019-11-08T03:30:33.396646: Epoch   4 Batch 1000/3125   train_loss = 1.012\n",
            "2019-11-08T03:30:34.473475: Epoch   4 Batch 1020/3125   train_loss = 0.963\n",
            "2019-11-08T03:30:35.466626: Epoch   4 Batch 1040/3125   train_loss = 0.759\n",
            "2019-11-08T03:30:36.495505: Epoch   4 Batch 1060/3125   train_loss = 0.953\n",
            "2019-11-08T03:30:37.470803: Epoch   4 Batch 1080/3125   train_loss = 0.904\n",
            "2019-11-08T03:30:38.506228: Epoch   4 Batch 1100/3125   train_loss = 0.863\n",
            "2019-11-08T03:30:39.562541: Epoch   4 Batch 1120/3125   train_loss = 0.882\n",
            "2019-11-08T03:30:40.615885: Epoch   4 Batch 1140/3125   train_loss = 0.882\n",
            "2019-11-08T03:30:41.653423: Epoch   4 Batch 1160/3125   train_loss = 0.780\n",
            "2019-11-08T03:30:42.698122: Epoch   4 Batch 1180/3125   train_loss = 0.861\n",
            "2019-11-08T03:30:43.752200: Epoch   4 Batch 1200/3125   train_loss = 1.005\n",
            "2019-11-08T03:30:44.818899: Epoch   4 Batch 1220/3125   train_loss = 0.934\n",
            "2019-11-08T03:30:45.885676: Epoch   4 Batch 1240/3125   train_loss = 0.797\n",
            "2019-11-08T03:30:46.950590: Epoch   4 Batch 1260/3125   train_loss = 0.902\n",
            "2019-11-08T03:30:47.984343: Epoch   4 Batch 1280/3125   train_loss = 0.890\n",
            "2019-11-08T03:30:49.032586: Epoch   4 Batch 1300/3125   train_loss = 0.799\n",
            "2019-11-08T03:30:50.058670: Epoch   4 Batch 1320/3125   train_loss = 0.898\n",
            "2019-11-08T03:30:51.126909: Epoch   4 Batch 1340/3125   train_loss = 0.725\n",
            "2019-11-08T03:30:52.133158: Epoch   4 Batch 1360/3125   train_loss = 0.838\n",
            "2019-11-08T03:30:53.207052: Epoch   4 Batch 1380/3125   train_loss = 0.809\n",
            "2019-11-08T03:30:54.195355: Epoch   4 Batch 1400/3125   train_loss = 0.924\n",
            "2019-11-08T03:30:55.227698: Epoch   4 Batch 1420/3125   train_loss = 0.869\n",
            "2019-11-08T03:30:56.276395: Epoch   4 Batch 1440/3125   train_loss = 0.753\n",
            "2019-11-08T03:30:57.353381: Epoch   4 Batch 1460/3125   train_loss = 0.903\n",
            "2019-11-08T03:30:58.402621: Epoch   4 Batch 1480/3125   train_loss = 0.848\n",
            "2019-11-08T03:30:59.489671: Epoch   4 Batch 1500/3125   train_loss = 0.946\n",
            "2019-11-08T03:31:00.557845: Epoch   4 Batch 1520/3125   train_loss = 0.830\n",
            "2019-11-08T03:31:01.612059: Epoch   4 Batch 1540/3125   train_loss = 1.022\n",
            "2019-11-08T03:31:02.654852: Epoch   4 Batch 1560/3125   train_loss = 0.819\n",
            "2019-11-08T03:31:03.711431: Epoch   4 Batch 1580/3125   train_loss = 1.020\n",
            "2019-11-08T03:31:04.785070: Epoch   4 Batch 1600/3125   train_loss = 0.840\n",
            "2019-11-08T03:31:05.860613: Epoch   4 Batch 1620/3125   train_loss = 0.801\n",
            "2019-11-08T03:31:06.911620: Epoch   4 Batch 1640/3125   train_loss = 0.901\n",
            "2019-11-08T03:31:07.924958: Epoch   4 Batch 1660/3125   train_loss = 1.006\n",
            "2019-11-08T03:31:08.924726: Epoch   4 Batch 1680/3125   train_loss = 0.856\n",
            "2019-11-08T03:31:09.895421: Epoch   4 Batch 1700/3125   train_loss = 0.775\n",
            "2019-11-08T03:31:10.912335: Epoch   4 Batch 1720/3125   train_loss = 0.858\n",
            "2019-11-08T03:31:11.967217: Epoch   4 Batch 1740/3125   train_loss = 0.912\n",
            "2019-11-08T03:31:13.022633: Epoch   4 Batch 1760/3125   train_loss = 0.882\n",
            "2019-11-08T03:31:14.070109: Epoch   4 Batch 1780/3125   train_loss = 0.884\n",
            "2019-11-08T03:31:15.098706: Epoch   4 Batch 1800/3125   train_loss = 0.836\n",
            "2019-11-08T03:31:16.139296: Epoch   4 Batch 1820/3125   train_loss = 0.896\n",
            "2019-11-08T03:31:17.206857: Epoch   4 Batch 1840/3125   train_loss = 0.956\n",
            "2019-11-08T03:31:18.278940: Epoch   4 Batch 1860/3125   train_loss = 0.951\n",
            "2019-11-08T03:31:19.287067: Epoch   4 Batch 1880/3125   train_loss = 0.887\n",
            "2019-11-08T03:31:20.345938: Epoch   4 Batch 1900/3125   train_loss = 0.749\n",
            "2019-11-08T03:31:21.383329: Epoch   4 Batch 1920/3125   train_loss = 0.862\n",
            "2019-11-08T03:31:22.431561: Epoch   4 Batch 1940/3125   train_loss = 0.762\n",
            "2019-11-08T03:31:23.431969: Epoch   4 Batch 1960/3125   train_loss = 0.767\n",
            "2019-11-08T03:31:24.506827: Epoch   4 Batch 1980/3125   train_loss = 0.924\n",
            "2019-11-08T03:31:25.560349: Epoch   4 Batch 2000/3125   train_loss = 1.021\n",
            "2019-11-08T03:31:26.655580: Epoch   4 Batch 2020/3125   train_loss = 0.959\n",
            "2019-11-08T03:31:27.682763: Epoch   4 Batch 2040/3125   train_loss = 0.795\n",
            "2019-11-08T03:31:28.727038: Epoch   4 Batch 2060/3125   train_loss = 0.821\n",
            "2019-11-08T03:31:29.790565: Epoch   4 Batch 2080/3125   train_loss = 1.016\n",
            "2019-11-08T03:31:30.805679: Epoch   4 Batch 2100/3125   train_loss = 0.798\n",
            "2019-11-08T03:31:31.829802: Epoch   4 Batch 2120/3125   train_loss = 0.835\n",
            "2019-11-08T03:31:32.872804: Epoch   4 Batch 2140/3125   train_loss = 0.832\n",
            "2019-11-08T03:31:33.948229: Epoch   4 Batch 2160/3125   train_loss = 0.835\n",
            "2019-11-08T03:31:34.977487: Epoch   4 Batch 2180/3125   train_loss = 0.879\n",
            "2019-11-08T03:31:36.022743: Epoch   4 Batch 2200/3125   train_loss = 0.806\n",
            "2019-11-08T03:31:37.007858: Epoch   4 Batch 2220/3125   train_loss = 0.819\n",
            "2019-11-08T03:31:38.086689: Epoch   4 Batch 2240/3125   train_loss = 0.828\n",
            "2019-11-08T03:31:39.159908: Epoch   4 Batch 2260/3125   train_loss = 0.862\n",
            "2019-11-08T03:31:40.261084: Epoch   4 Batch 2280/3125   train_loss = 0.932\n",
            "2019-11-08T03:31:41.281683: Epoch   4 Batch 2300/3125   train_loss = 0.814\n",
            "2019-11-08T03:31:42.417716: Epoch   4 Batch 2320/3125   train_loss = 0.930\n",
            "2019-11-08T03:31:43.508614: Epoch   4 Batch 2340/3125   train_loss = 0.852\n",
            "2019-11-08T03:31:44.529779: Epoch   4 Batch 2360/3125   train_loss = 0.881\n",
            "2019-11-08T03:31:45.601025: Epoch   4 Batch 2380/3125   train_loss = 0.824\n",
            "2019-11-08T03:31:46.653800: Epoch   4 Batch 2400/3125   train_loss = 1.017\n",
            "2019-11-08T03:31:47.705590: Epoch   4 Batch 2420/3125   train_loss = 0.804\n",
            "2019-11-08T03:31:48.738654: Epoch   4 Batch 2440/3125   train_loss = 0.851\n",
            "2019-11-08T03:31:49.815932: Epoch   4 Batch 2460/3125   train_loss = 0.838\n",
            "2019-11-08T03:31:50.886860: Epoch   4 Batch 2480/3125   train_loss = 0.986\n",
            "2019-11-08T03:31:51.953406: Epoch   4 Batch 2500/3125   train_loss = 0.790\n",
            "2019-11-08T03:31:53.002109: Epoch   4 Batch 2520/3125   train_loss = 0.933\n",
            "2019-11-08T03:31:54.037151: Epoch   4 Batch 2540/3125   train_loss = 0.835\n",
            "2019-11-08T03:31:55.113385: Epoch   4 Batch 2560/3125   train_loss = 0.676\n",
            "2019-11-08T03:31:56.183684: Epoch   4 Batch 2580/3125   train_loss = 0.885\n",
            "2019-11-08T03:31:57.223540: Epoch   4 Batch 2600/3125   train_loss = 0.866\n",
            "2019-11-08T03:31:58.290613: Epoch   4 Batch 2620/3125   train_loss = 0.807\n",
            "2019-11-08T03:31:59.351259: Epoch   4 Batch 2640/3125   train_loss = 0.852\n",
            "2019-11-08T03:32:00.433013: Epoch   4 Batch 2660/3125   train_loss = 0.992\n",
            "2019-11-08T03:32:01.483235: Epoch   4 Batch 2680/3125   train_loss = 0.807\n",
            "2019-11-08T03:32:02.543319: Epoch   4 Batch 2700/3125   train_loss = 0.878\n",
            "2019-11-08T03:32:03.536675: Epoch   4 Batch 2720/3125   train_loss = 0.751\n",
            "2019-11-08T03:32:04.609409: Epoch   4 Batch 2740/3125   train_loss = 0.880\n",
            "2019-11-08T03:32:05.687268: Epoch   4 Batch 2760/3125   train_loss = 0.782\n",
            "2019-11-08T03:32:06.719902: Epoch   4 Batch 2780/3125   train_loss = 0.859\n",
            "2019-11-08T03:32:07.756166: Epoch   4 Batch 2800/3125   train_loss = 1.004\n",
            "2019-11-08T03:32:08.818020: Epoch   4 Batch 2820/3125   train_loss = 1.018\n",
            "2019-11-08T03:32:09.823210: Epoch   4 Batch 2840/3125   train_loss = 0.869\n",
            "2019-11-08T03:32:10.884321: Epoch   4 Batch 2860/3125   train_loss = 0.799\n",
            "2019-11-08T03:32:11.906755: Epoch   4 Batch 2880/3125   train_loss = 0.878\n",
            "2019-11-08T03:32:12.926916: Epoch   4 Batch 2900/3125   train_loss = 0.888\n",
            "2019-11-08T03:32:14.006660: Epoch   4 Batch 2920/3125   train_loss = 0.852\n",
            "2019-11-08T03:32:15.101844: Epoch   4 Batch 2940/3125   train_loss = 0.946\n",
            "2019-11-08T03:32:16.203458: Epoch   4 Batch 2960/3125   train_loss = 0.924\n",
            "2019-11-08T03:32:17.273487: Epoch   4 Batch 2980/3125   train_loss = 0.866\n",
            "2019-11-08T03:32:18.284540: Epoch   4 Batch 3000/3125   train_loss = 0.949\n",
            "2019-11-08T03:32:19.317496: Epoch   4 Batch 3020/3125   train_loss = 1.013\n",
            "2019-11-08T03:32:20.348150: Epoch   4 Batch 3040/3125   train_loss = 0.895\n",
            "2019-11-08T03:32:21.432020: Epoch   4 Batch 3060/3125   train_loss = 0.779\n",
            "2019-11-08T03:32:22.507072: Epoch   4 Batch 3080/3125   train_loss = 0.972\n",
            "2019-11-08T03:32:23.556757: Epoch   4 Batch 3100/3125   train_loss = 1.005\n",
            "2019-11-08T03:32:24.512355: Epoch   4 Batch 3120/3125   train_loss = 0.860\n",
            "2019-11-08T03:32:24.889173: Epoch   4 Batch   16/781   test_loss = 0.829\n",
            "2019-11-08T03:32:25.128200: Epoch   4 Batch   36/781   test_loss = 0.905\n",
            "2019-11-08T03:32:25.344047: Epoch   4 Batch   56/781   test_loss = 0.903\n",
            "2019-11-08T03:32:25.575817: Epoch   4 Batch   76/781   test_loss = 1.001\n",
            "2019-11-08T03:32:25.786649: Epoch   4 Batch   96/781   test_loss = 0.978\n",
            "2019-11-08T03:32:26.006909: Epoch   4 Batch  116/781   test_loss = 0.880\n",
            "2019-11-08T03:32:26.220538: Epoch   4 Batch  136/781   test_loss = 0.818\n",
            "2019-11-08T03:32:26.448236: Epoch   4 Batch  156/781   test_loss = 0.890\n",
            "2019-11-08T03:32:26.703127: Epoch   4 Batch  176/781   test_loss = 0.876\n",
            "2019-11-08T03:32:26.944019: Epoch   4 Batch  196/781   test_loss = 0.831\n",
            "2019-11-08T03:32:27.180102: Epoch   4 Batch  216/781   test_loss = 0.918\n",
            "2019-11-08T03:32:27.424468: Epoch   4 Batch  236/781   test_loss = 0.738\n",
            "2019-11-08T03:32:27.688376: Epoch   4 Batch  256/781   test_loss = 0.852\n",
            "2019-11-08T03:32:27.950583: Epoch   4 Batch  276/781   test_loss = 1.041\n",
            "2019-11-08T03:32:28.221381: Epoch   4 Batch  296/781   test_loss = 0.848\n",
            "2019-11-08T03:32:28.450065: Epoch   4 Batch  316/781   test_loss = 0.823\n",
            "2019-11-08T03:32:28.691703: Epoch   4 Batch  336/781   test_loss = 0.777\n",
            "2019-11-08T03:32:28.913668: Epoch   4 Batch  356/781   test_loss = 0.866\n",
            "2019-11-08T03:32:29.133823: Epoch   4 Batch  376/781   test_loss = 0.899\n",
            "2019-11-08T03:32:29.360439: Epoch   4 Batch  396/781   test_loss = 0.835\n",
            "2019-11-08T03:32:29.575574: Epoch   4 Batch  416/781   test_loss = 0.914\n",
            "2019-11-08T03:32:29.790949: Epoch   4 Batch  436/781   test_loss = 0.900\n",
            "2019-11-08T03:32:30.015449: Epoch   4 Batch  456/781   test_loss = 0.725\n",
            "2019-11-08T03:32:30.243187: Epoch   4 Batch  476/781   test_loss = 0.917\n",
            "2019-11-08T03:32:30.460491: Epoch   4 Batch  496/781   test_loss = 0.969\n",
            "2019-11-08T03:32:30.680396: Epoch   4 Batch  516/781   test_loss = 0.780\n",
            "2019-11-08T03:32:30.886709: Epoch   4 Batch  536/781   test_loss = 0.924\n",
            "2019-11-08T03:32:31.098392: Epoch   4 Batch  556/781   test_loss = 0.836\n",
            "2019-11-08T03:32:31.317903: Epoch   4 Batch  576/781   test_loss = 0.951\n",
            "2019-11-08T03:32:31.544504: Epoch   4 Batch  596/781   test_loss = 0.963\n",
            "2019-11-08T03:32:31.783950: Epoch   4 Batch  616/781   test_loss = 0.988\n",
            "2019-11-08T03:32:32.010250: Epoch   4 Batch  636/781   test_loss = 0.835\n",
            "2019-11-08T03:32:32.238029: Epoch   4 Batch  656/781   test_loss = 0.876\n",
            "2019-11-08T03:32:32.475629: Epoch   4 Batch  676/781   test_loss = 1.067\n",
            "2019-11-08T03:32:32.744346: Epoch   4 Batch  696/781   test_loss = 0.877\n",
            "2019-11-08T03:32:32.986877: Epoch   4 Batch  716/781   test_loss = 0.896\n",
            "2019-11-08T03:32:33.244576: Epoch   4 Batch  736/781   test_loss = 1.078\n",
            "2019-11-08T03:32:33.494049: Epoch   4 Batch  756/781   test_loss = 0.863\n",
            "2019-11-08T03:32:33.722595: Epoch   4 Batch  776/781   test_loss = 0.721\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl6mGeQTOxFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_params((save_dir))\n",
        "\n",
        "load_dir = load_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnGoZGfZO63d",
        "colab_type": "code",
        "outputId": "732f2156-1ca2-43cd-a0f3-2e889cad9f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(losses['train'], label='Training loss')\n",
        "plt.legend()\n",
        "_ = plt.ylim()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvIAAAHwCAYAAADEu4vaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 377,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "198nUffeO9qB",
        "colab_type": "code",
        "outputId": "1d736b85-861f-4098-a85a-bef9cc7465e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(losses['test'], label='Test loss')\n",
        "plt.legend()\n",
        "_ = plt.ylim()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvIAAAHwCAYAAADEu4vaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 377,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwv3lZk1qkvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2DSyIdbPI8V",
        "colab_type": "text"
      },
      "source": [
        "# Get tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDxXccMAPDlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#使用函数 get_tensor_by_name()从 loaded_graph 中获取tensors，后面的推荐功能要用到\n",
        "def get_tensors(loaded_graph):\n",
        "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
        "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
        "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
        "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
        "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
        "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
        "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
        "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
        "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
        "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
        "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\")\n",
        "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
        "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
        "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AEM_Zd6PQG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rating_movie(user_id_val, movie_id_val):\n",
        "    loaded_graph = tf.Graph()  #\n",
        "    with tf.Session(graph=loaded_graph) as sess:  #\n",
        "        # Load saved model\n",
        "        loader = tf.train.import_meta_graph(load_dir + '.meta')   #由于已经将模型保存在了 .meta 文件中，因此可使用tf.train.import()函数来重新创建网络\n",
        "                                                                #使用别人已经训练好的模型来fine-tuning的第一步：此为创建网络Create the network\n",
        "        loader.restore(sess, load_dir)                          #第二步：加载参数Load the parameters，调用restore函数来恢复网络的参数\n",
        "    \n",
        "        # Get Tensors from loaded model\n",
        "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
        "    \n",
        "        categories = np.zeros([1, 18])\n",
        "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
        "    \n",
        "        titles = np.zeros([1, sentences_size])\n",
        "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
        "    \n",
        "        feed = {\n",
        "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
        "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
        "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
        "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
        "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
        "              movie_categories: categories,  #x.take(6,1)\n",
        "              movie_titles: titles,  #x.take(5,1)\n",
        "              dropout_keep_prob: 1}\n",
        "    \n",
        "        # Get Prediction\n",
        "        inference_val = sess.run([inference], feed)  \n",
        "    \n",
        "        return (inference_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijAEP4djPXho",
        "colab_type": "code",
        "outputId": "c5bb4793-2ac0-4d0f-d58b-fd6039d5576c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rating_movie(234, 1401)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[3.672011]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wMIOxs0PaBk",
        "colab_type": "code",
        "outputId": "c5c8e950-44f5-4aec-d70a-9d5b191c6b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_graph = tf.Graph()  #1、新建一个在这段代码中的graph\n",
        "movie_matrics = []\n",
        "with tf.Session(graph=loaded_graph) as sess:  #2、在session中引入这个图\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
        "    loader.restore(sess, load_dir)\n",
        "\n",
        "    # Get Tensors from loaded model\n",
        "    # 要恢复这个网络，不仅需要恢复图（graph）和权重，也需要准备一个新的feed_dict，将新的训练数据喂给网络。\n",
        "    #我们可以通过使用graph.get_tensor_by_name()方法来获得已经保存的操作（operations）和placeholder variables。\n",
        "    #为后续feed做准备\n",
        "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
        "\n",
        "    for item in movies.values:\n",
        "        categories = np.zeros([1, 18])\n",
        "        categories[0] = item.take(2)\n",
        "\n",
        "        titles = np.zeros([1, sentences_size])\n",
        "        titles[0] = item.take(1)\n",
        "\n",
        "        feed = {\n",
        "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
        "            movie_categories: categories,  #x.take(6,1)\n",
        "            movie_titles: titles,  #x.take(5,1)\n",
        "            dropout_keep_prob: 1}\n",
        "\n",
        "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)    #执行整个movie结构中的最后一个功能,完成全部的数据流动,得到输出的电影特征\n",
        "        movie_matrics.append(movie_combine_layer_flat_val)  #为每个movie生成一个电影特征矩阵，存储到movie_matrics列表中\n",
        "\n",
        "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
        "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv2LLOmGPeYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrK0zS3gPltY",
        "colab_type": "text"
      },
      "source": [
        "# user matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS82xqcjPgfu",
        "colab_type": "code",
        "outputId": "1401af83-83bf-4cbb-9881-4f617282c4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#将训练好的用户特征组合成用户特征矩阵并保存到本地\n",
        "loaded_graph = tf.Graph()  # #1、新建一个graph\n",
        "users_matrics = []\n",
        "with tf.Session(graph=loaded_graph) as sess:  #\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
        "    loader.restore(sess, load_dir)\n",
        "\n",
        "    # Get Tensors from loaded model\n",
        "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
        "\n",
        "    for item in users.values:\n",
        "\n",
        "        feed = {\n",
        "            uid: np.reshape(item.take(0), [1, 1]),\n",
        "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
        "            user_age: np.reshape(item.take(2), [1, 1]),\n",
        "            user_job: np.reshape(item.take(3), [1, 1]),\n",
        "            dropout_keep_prob: 1}\n",
        "\n",
        "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
        "        users_matrics.append(user_combine_layer_flat_val)\n",
        "\n",
        "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
        "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOwdH2EhPopH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR4BmR2PPwPu",
        "colab_type": "text"
      },
      "source": [
        "# recommendation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaEbKkQjPrSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#使用生产的用户特征矩阵和电影特征矩阵做电影推荐\n",
        "\n",
        "# 1、推荐同类型的电影\n",
        "# 思路是计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个\n",
        "#这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。\n",
        "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
        "    \n",
        "    loaded_graph = tf.Graph()  #\n",
        "    with tf.Session(graph=loaded_graph) as sess:  #\n",
        "        # Load saved model\n",
        "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
        "        loader.restore(sess, load_dir)\n",
        "        \n",
        "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
        "                            #movie_matrics显示为 (3883, 200)\n",
        "                            #array([[-0.9784413 ,  0.97033578, -0.99996817, ..., -0.94367135,0.938721  ,  0.94092846],...])\n",
        " \n",
        "                            #tf.square()是对a里的每一个元素求平方i=(x,y)\n",
        "                            #tf.reduce_sum,注意参数表示在维度1(列)上进行求和,且维度不变 x^2+y^2\n",
        "                            #tf.sqrt计算x元素的平方根\n",
        "                            #这里完成向量单位化\n",
        "                            #(3883, 1\n",
        "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
        "\n",
        "        #推荐同类型的电影\n",
        "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200]) #用户输入已看过的电影，进行movieid2idx数字转化\n",
        "                                                                                        #movie_matrics[转化后的标记数值]得到对应的电影特征向量\n",
        "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))  #即得到输入的电影与各个电影的余弦相似性的值\n",
        "                                                                                              #(1,200)×(200,3883)\n",
        "        sim = (probs_similarity.eval())\n",
        "        #转化为字符串\n",
        "        #sim [[ 13.49374485  13.48943233  13.51107979 ...,  13.50281906  13.49236774  13.49707603]]\n",
        " \n",
        "    #     results = (-sim[0]).argsort()[0:top_k]\n",
        "    #     print(results)\n",
        "        \n",
        "        print(\"您看的电影是：{}\".format(movies_orig[[movie_id_val]]))    #movies_orig原始未处理的电影数据，为输出用户可读\n",
        "        print(\"以下是给您的推荐：\")\n",
        "        p = np.squeeze(sim)   #np.squeeze将表示向量的数组转换为秩为1的数组\n",
        "        p[np.argsort(p)[:-top_k]] = 0     #numpy.argsort()\n",
        "                                      #x=np.array([1,4,3,-1,6,9])\n",
        "                                      #函数含义：首先将p中的元素从小到大排列后，得到[-1,1,3,4,6,9]\n",
        "                                      #          按照所得的排好序的对应找其在原x中的索引值，如-1由x[3]得到；1由x[0]得到，所以索引值为[3,0,2,1,4,5]\n",
        "                                      #          所以这个即为输出\n",
        "                                      #np.argsort()[:-top_k]表示将np.argsort()得到的结果去掉后面20个后的前面所有值为0，因为我们只考虑最相似的20个\n",
        "                                      #这些值不为0，以便做后面的处理\n",
        "        p = p / np.sum(p)   #sum函数对某一维度求和，这里表示全部元素求和,这里将p的值限制在0~1\n",
        "        results = set()\n",
        "        while len(results) != 5:\n",
        "            c = np.random.choice(3883, 1, p=p)[0]     #参数意思分别 是从a 中以概率P，随机选择3个,\n",
        "                                                  #p没有指定的时候表示同等概率会被取出，p指定时表示每个数会被取出的概率\n",
        "                                                  #replace代表的意思是抽样之后不放回，选出的三个数都不一样\n",
        "                                                  #a1 = np.random.choice(a=5, size=3, replace=False, p=None)\n",
        "            results.add(c)        #results本身为set（可以完成剔除掉相同的推荐，虽然前面np.random.choice是不放回）\n",
        "        for val in (results):\n",
        "            print(val)      #由于前面已经转换为字符串eval，所以可以直接输出\n",
        "            print(movies_orig[val])\n",
        "        \n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIOkgbePPzca",
        "colab_type": "code",
        "outputId": "957c983e-e458-4d01-8994-fc83d20283b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "recommend_same_type_movie(1401, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n",
            "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
            "以下是给您的推荐：\n",
            "864\n",
            "[875 'Nothing to Lose (1994)' 'Drama']\n",
            "2570\n",
            "[2639 'Mommie Dearest (1981)' 'Drama']\n",
            "2859\n",
            "[2928 \"Razor's Edge, The (1984)\" 'Drama']\n",
            "945\n",
            "[957 'Scarlet Letter, The (1926)' 'Drama']\n",
            "371\n",
            "[375 'Safe Passage (1994)' 'Drama']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{371, 864, 945, 2570, 2859}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G6ipXKLP2Lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2、推荐您喜欢的电影\n",
        "#思路是使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的top_k个，同样加了些随机选择\n",
        "\n",
        "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
        "\n",
        "    loaded_graph = tf.Graph()  #\n",
        "    with tf.Session(graph=loaded_graph) as sess:  #\n",
        "        # Load saved model\n",
        "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
        "        loader.restore(sess, load_dir)\n",
        "\n",
        "        #推荐您喜欢的电影\n",
        "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])   #！！！这里变成用户特征，且前面没有余弦相似性的计算\n",
        "\n",
        "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))   #这里计算后的结果就是预测分数，相当于模型中计算inference \n",
        "        sim = (probs_similarity.eval())\n",
        "    #     print(sim.shape)\n",
        "    #     results = (-sim[0]).argsort()[0:top_k]\n",
        "    #     print(results)\n",
        "        \n",
        "    #     sim_norm = probs_norm_similarity.eval()\n",
        "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
        "    \n",
        "        print(\"以下是给您的推荐：\")\n",
        "        p = np.squeeze(sim)\n",
        "        p[np.argsort(p)[:-top_k]] = 0\n",
        "        p = p / np.sum(p)\n",
        "        results = set()\n",
        "        while len(results) != 5:\n",
        "            c = np.random.choice(3883, 1, p=p)[0]\n",
        "            results.add(c)\n",
        "        for val in (results):\n",
        "            print(val)\n",
        "            print(movies_orig[val])\n",
        "\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH8lQGZJP6c4",
        "colab_type": "code",
        "outputId": "10ca5ba7-aa20-42dc-8798-25fb1733d8ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "recommend_your_favorite_movie(234, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n",
            "以下是给您的推荐：\n",
            "900\n",
            "[912 'Casablanca (1942)' 'Drama|Romance|War']\n",
            "1674\n",
            "[1723 'Twisted (1996)' 'Comedy|Drama']\n",
            "3124\n",
            "[3193 'Creature (1999)' 'Documentary']\n",
            "1244\n",
            "[1264 'Diva (1981)' 'Action|Drama|Mystery|Romance|Thriller']\n",
            "735\n",
            "[745 'Close Shave, A (1995)' 'Animation|Comedy|Thriller']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{735, 900, 1244, 1674, 3124}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ2twYvaQEdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3、看过这个电影的人还看了（喜欢）哪些电影\n",
        " #首先选出喜欢某个电影的top_k个人，得到这几个人的用户特征向量。\n",
        " #然后计算这几个人对所有电影的评分\n",
        " #选择每个人评分最高的电影作为推荐\n",
        " #同样加入了随机选择\n",
        "\n",
        "import random\n",
        "\n",
        "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
        "    loaded_graph = tf.Graph()  #\n",
        "    with tf.Session(graph=loaded_graph) as sess:  #\n",
        "        # Load saved model\n",
        "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
        "        loader.restore(sess, load_dir)\n",
        "\n",
        "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])   #根据输入的电影得到这个电影的特征向量\n",
        "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
        "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]        #选出喜欢某个电影的top_k个人\n",
        "    #     print(normalized_users_matrics.eval().shape)\n",
        "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
        "    #     print(favorite_user_id.shape)\n",
        "    \n",
        "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
        "        \n",
        "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
        "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])   #计算这几个人的特征\n",
        "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
        "        sim = (probs_similarity.eval())\n",
        "    #     results = (-sim[0]).argsort()[0:top_k]\n",
        "    #     print(results)\n",
        "    \n",
        "    #     print(sim.shape)\n",
        "    #     print(np.argmax(sim, 1))\n",
        "        p = np.argmax(sim, 1)\n",
        "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
        "\n",
        "        results = set()\n",
        "        while len(results) != 5:\n",
        "            c = p[random.randrange(top_k)]\n",
        "            results.add(c)\n",
        "        for val in (results):\n",
        "            print(val)\n",
        "            print(movies_orig[val])\n",
        "        \n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ummDziX4QIln",
        "colab_type": "code",
        "outputId": "9cdcc339-7672-42f4-f5a0-72d37d305085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "recommend_other_favorite_movie(1401, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n",
            "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
            "喜欢看这个电影的人是：[[4814 'M' 18 14 '80526']\n",
            " [2851 'F' 25 7 '94107']\n",
            " [985 'M' 25 4 '32608']\n",
            " [899 'F' 25 1 '30605']\n",
            " [900 'F' 56 13 '90066']\n",
            " [1763 'M' 35 7 '76248']\n",
            " [5214 'F' 35 9 '07452']\n",
            " [3095 'F' 25 3 '06810']\n",
            " [100 'M' 35 17 '95401']\n",
            " [4085 'F' 25 6 '79416']\n",
            " [3150 'F' 25 3 '92831']\n",
            " [5055 'F' 35 16 '97330']\n",
            " [5861 'F' 50 1 '98499']\n",
            " [4142 'M' 56 7 '01040']\n",
            " [3676 'F' 35 12 '48109']\n",
            " [3031 'M' 18 4 '48135']\n",
            " [2391 'M' 50 18 '13126']\n",
            " [371 'M' 18 4 '02141']\n",
            " [2338 'M' 45 17 '13152']\n",
            " [5767 'M' 25 2 '75287']]\n",
            "喜欢看这个电影的人还喜欢看：\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}